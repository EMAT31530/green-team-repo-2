{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing modules for the implementation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from matplotlib import pyplot\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "nutrition_test = pd.read_csv(\"Test_20.csv\")\n",
    "nutrition_train = pd.read_csv(\"Train_80.csv\")\n",
    "nutrition_test = nutrition_test.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "nutrition_train = nutrition_train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "nutrition_test = nutrition_test.dropna()\n",
    "nutrition_train = nutrition_train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the nutrition data, labelling X as this is the input to the SKlearn algorithm\n",
    "X_train = nutrition_train.iloc[:,2:42]\n",
    "nutrition_titles = X_train.columns\n",
    "X_test = nutrition_test.iloc[:,2:42]\n",
    "#extracting the greenhouse gas emissions \n",
    "y_test = nutrition_test.iloc[:,43:44]\n",
    "y_train = nutrition_train.iloc[:,43:44]\n",
    "y_test = np.ravel(y_test)\n",
    "y_train = np.ravel(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LGBMRegressor()\n"
     ]
    }
   ],
   "source": [
    "model = lgb.LGBMRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "print(); print(model)\n",
    "\n",
    "expected_y  = y_test\n",
    "predicted_y = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8840492626454673\n",
      "0.25103951919082945\n"
     ]
    }
   ],
   "source": [
    "print(metrics.r2_score(expected_y, predicted_y))\n",
    "print(metrics.mean_absolute_error(expected_y, predicted_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    # Parameters that i am going to tune.\n",
    "    'max_depth':6,\n",
    "    'num_leaf': 4,\n",
    "    'learning_rate':.05,\n",
    "    'max_bin': 255,\n",
    "    'min_data_in_leaf': 4,\n",
    "    # Other parameters\n",
    "    'objective':'regression',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numboost and early stopping rounds for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000993 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6839\n",
      "[LightGBM] [Info] Number of data points in the train set: 832, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.868784\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "train_data=lgb.Dataset(X_train,label=y_train)\n",
    "test_data=lgb.Dataset(X_test,label=y_test)\n",
    "# evaluation metric\n",
    "#params['eval_metric'] = \"mae\"\n",
    "# num boosting rounds is set to a high number as we want to allow time \n",
    "# for the optimal boosting round to be found. \n",
    "num_boost_round = 999\n",
    "#training our model using light gbm\n",
    "num_round=50\n",
    "lgbm=lgb.train(params,train_data,num_round)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6839\n",
      "[LightGBM] [Info] Number of data points in the train set: 832, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.868784\n"
     ]
    }
   ],
   "source": [
    "#training our model using light gbm\n",
    "num_round=50\n",
    "start=datetime.now()\n",
    "lgbm=lgb.train(params,train_data,num_round)\n",
    "stop=datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(microseconds=36895)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Execution time of the model\n",
    "execution_time_lgbm = stop-start\n",
    "execution_time_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing performance\n",
      "RMSE: 0.63\n",
      "R2: 0.79\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "#predicting on test set\n",
    "pred=lgbm.predict(X_test)\n",
    "rmse = (np.sqrt(mean_squared_error(y_test, pred)))\n",
    "r2 = r2_score(y_test, pred)\n",
    "print(\"Testing performance\")\n",
    "print('RMSE: {:.2f}'.format(rmse))\n",
    "print('R2: {:.2f}'.format(r2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempted to set up a random hyperparameter tuner, unsuccessful, i think because of the size of the data set, the model is overfitting really quickly, or that the num of leaves is too big. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number 0\n",
      "{'learning_rate': 0.5977991198051446, 'boosting_type': 'dart', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.7801180115006612, 'num_leaves': 2, 'min_data': 8, 'max_depth': 149} 9862\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001215 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6839\n",
      "[LightGBM] [Info] Number of data points in the train set: 832, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.868784\n",
      "failed with\n",
      "{'learning_rate': 0.5977991198051446, 'boosting_type': 'dart', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.7801180115006612, 'num_leaves': 2, 'min_data': 8, 'max_depth': 149}\n",
      "**************************************************\n",
      "Minimum is:  99999999999999999999999\n",
      "iteration number 1\n",
      "{'learning_rate': 0.721013387753628, 'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.1360801957356984, 'num_leaves': 10, 'min_data': 21, 'max_depth': 164} 3890\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001039 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6839\n",
      "[LightGBM] [Info] Number of data points in the train set: 832, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.868784\n",
      "failed with\n",
      "{'learning_rate': 0.721013387753628, 'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.1360801957356984, 'num_leaves': 10, 'min_data': 21, 'max_depth': 164}\n",
      "**************************************************\n",
      "Minimum is:  99999999999999999999999\n",
      "iteration number 2\n",
      "{'learning_rate': 0.527038151457817, 'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.27047736334595895, 'num_leaves': 4, 'min_data': 19, 'max_depth': 42} 7728\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001006 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6839\n",
      "[LightGBM] [Info] Number of data points in the train set: 832, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.868784\n",
      "failed with\n",
      "{'learning_rate': 0.527038151457817, 'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.27047736334595895, 'num_leaves': 4, 'min_data': 19, 'max_depth': 42}\n",
      "**************************************************\n",
      "Minimum is:  99999999999999999999999\n",
      "iteration number 3\n",
      "{'learning_rate': 0.39160530535344906, 'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.28896282998819867, 'num_leaves': 13, 'min_data': 16, 'max_depth': 35} 3643\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001061 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6839\n",
      "[LightGBM] [Info] Number of data points in the train set: 832, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.868784\n",
      "failed with\n",
      "{'learning_rate': 0.39160530535344906, 'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.28896282998819867, 'num_leaves': 13, 'min_data': 16, 'max_depth': 35}\n",
      "**************************************************\n",
      "Minimum is:  99999999999999999999999\n",
      "iteration number 4\n",
      "{'learning_rate': 0.33246440636556696, 'boosting_type': 'goss', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.6610647986932886, 'num_leaves': 4, 'min_data': 21, 'max_depth': 42} 3517\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001161 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6839\n",
      "[LightGBM] [Info] Number of data points in the train set: 832, number of used features: 40\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.868784\n",
      "failed with\n",
      "{'learning_rate': 0.33246440636556696, 'boosting_type': 'goss', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.6610647986932886, 'num_leaves': 4, 'min_data': 21, 'max_depth': 42}\n",
      "**************************************************\n",
      "Minimum is:  99999999999999999999999\n",
      "iteration number 5\n",
      "{'learning_rate': 0.2932091935402187, 'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.9474443647567062, 'num_leaves': 6, 'min_data': 10, 'max_depth': 182} 4550\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001023 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6839\n",
      "[LightGBM] [Info] Number of data points in the train set: 832, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.868784\n",
      "failed with\n",
      "{'learning_rate': 0.2932091935402187, 'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.9474443647567062, 'num_leaves': 6, 'min_data': 10, 'max_depth': 182}\n",
      "**************************************************\n",
      "Minimum is:  99999999999999999999999\n",
      "iteration number 6\n",
      "{'learning_rate': 0.30226975862051797, 'boosting_type': 'dart', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.3104341496310027, 'num_leaves': 14, 'min_data': 26, 'max_depth': 49} 8755\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001076 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6839\n",
      "[LightGBM] [Info] Number of data points in the train set: 832, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.868784\n",
      "failed with\n",
      "{'learning_rate': 0.30226975862051797, 'boosting_type': 'dart', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.3104341496310027, 'num_leaves': 14, 'min_data': 26, 'max_depth': 49}\n",
      "**************************************************\n",
      "Minimum is:  99999999999999999999999\n",
      "iteration number 7\n",
      "{'learning_rate': 0.9193106295358393, 'boosting_type': 'dart', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.48986545696011996, 'num_leaves': 10, 'min_data': 3, 'max_depth': 74} 8079\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001259 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6839\n",
      "[LightGBM] [Info] Number of data points in the train set: 832, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.868784\n",
      "failed with\n",
      "{'learning_rate': 0.9193106295358393, 'boosting_type': 'dart', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.48986545696011996, 'num_leaves': 10, 'min_data': 3, 'max_depth': 74}\n",
      "**************************************************\n",
      "Minimum is:  99999999999999999999999\n",
      "iteration number 8\n",
      "{'learning_rate': 0.3489406952927113, 'boosting_type': 'goss', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.6497887339049976, 'num_leaves': 13, 'min_data': 5, 'max_depth': 67} 6376\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001205 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6839\n",
      "[LightGBM] [Info] Number of data points in the train set: 832, number of used features: 40\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.868784\n",
      "failed with\n",
      "{'learning_rate': 0.3489406952927113, 'boosting_type': 'goss', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.6497887339049976, 'num_leaves': 13, 'min_data': 5, 'max_depth': 67}\n",
      "**************************************************\n",
      "Minimum is:  99999999999999999999999\n",
      "iteration number 9\n",
      "{'learning_rate': 0.7259732685129853, 'boosting_type': 'dart', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.20068257489956365, 'num_leaves': 1, 'min_data': 16, 'max_depth': 94} 7305\n",
      "failed with\n",
      "{'learning_rate': 0.7259732685129853, 'boosting_type': 'dart', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.20068257489956365, 'num_leaves': 1, 'min_data': 16, 'max_depth': 94}\n",
      "**************************************************\n",
      "Minimum is:  99999999999999999999999\n",
      "iteration number 10\n",
      "{'learning_rate': 0.13752484624885397, 'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.7656988127378453, 'num_leaves': 5, 'min_data': 14, 'max_depth': 148} 8955\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001066 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6839\n",
      "[LightGBM] [Info] Number of data points in the train set: 832, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.868784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed with\n",
      "{'learning_rate': 0.13752484624885397, 'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.7656988127378453, 'num_leaves': 5, 'min_data': 14, 'max_depth': 148}\n",
      "**************************************************\n",
      "Minimum is:  99999999999999999999999\n",
      "iteration number 11\n",
      "{'learning_rate': 0.324489888615771, 'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.08144874127767421, 'num_leaves': 3, 'min_data': 21, 'max_depth': 54} 3386\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000272 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6839\n",
      "[LightGBM] [Info] Number of data points in the train set: 832, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.868784\n",
      "failed with\n",
      "{'learning_rate': 0.324489888615771, 'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.08144874127767421, 'num_leaves': 3, 'min_data': 21, 'max_depth': 54}\n",
      "**************************************************\n",
      "Minimum is:  99999999999999999999999\n",
      "iteration number 12\n",
      "{'learning_rate': 0.9018344389815829, 'boosting_type': 'dart', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.25117459521814, 'num_leaves': 2, 'min_data': 24, 'max_depth': 14} 7216\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001111 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6839\n",
      "[LightGBM] [Info] Number of data points in the train set: 832, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.868784\n",
      "failed with\n",
      "{'learning_rate': 0.9018344389815829, 'boosting_type': 'dart', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.25117459521814, 'num_leaves': 2, 'min_data': 24, 'max_depth': 14}\n",
      "**************************************************\n",
      "Minimum is:  99999999999999999999999\n",
      "iteration number 13\n",
      "{'learning_rate': 0.7641033779103628, 'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.8852726973788658, 'num_leaves': 8, 'min_data': 27, 'max_depth': 182} 3814\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001237 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6839\n",
      "[LightGBM] [Info] Number of data points in the train set: 832, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.868784\n",
      "failed with\n",
      "{'learning_rate': 0.7641033779103628, 'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.8852726973788658, 'num_leaves': 8, 'min_data': 27, 'max_depth': 182}\n",
      "**************************************************\n",
      "Minimum is:  99999999999999999999999\n",
      "iteration number 14\n",
      "{'learning_rate': 0.3279885738330026, 'boosting_type': 'dart', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.5141389491636371, 'num_leaves': 9, 'min_data': 20, 'max_depth': 79} 4667\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6839\n",
      "[LightGBM] [Info] Number of data points in the train set: 832, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.868784\n",
      "failed with\n",
      "{'learning_rate': 0.3279885738330026, 'boosting_type': 'dart', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.5141389491636371, 'num_leaves': 9, 'min_data': 20, 'max_depth': 79}\n",
      "**************************************************\n",
      "Minimum is:  99999999999999999999999\n",
      "iteration number 15\n",
      "{'learning_rate': 0.17668765835985778, 'boosting_type': 'goss', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.08448710211894828, 'num_leaves': 14, 'min_data': 10, 'max_depth': 46} 1410\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6839\n",
      "[LightGBM] [Info] Number of data points in the train set: 832, number of used features: 40\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.868784\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "failed with\n",
      "{'learning_rate': 0.17668765835985778, 'boosting_type': 'goss', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.08448710211894828, 'num_leaves': 14, 'min_data': 10, 'max_depth': 46}\n",
      "**************************************************\n",
      "Minimum is:  99999999999999999999999\n",
      "iteration number 16\n",
      "{'learning_rate': 0.486611339228638, 'boosting_type': 'dart', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.9917679384110376, 'num_leaves': 4, 'min_data': 3, 'max_depth': 164} 327\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6839\n",
      "[LightGBM] [Info] Number of data points in the train set: 832, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.868784\n",
      "failed with\n",
      "{'learning_rate': 0.486611339228638, 'boosting_type': 'dart', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.9917679384110376, 'num_leaves': 4, 'min_data': 3, 'max_depth': 164}\n",
      "**************************************************\n",
      "Minimum is:  99999999999999999999999\n",
      "iteration number 17\n",
      "{'learning_rate': 0.34151260140761763, 'boosting_type': 'goss', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.5364782124240217, 'num_leaves': 11, 'min_data': 8, 'max_depth': 108} 2184\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001417 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6839\n",
      "[LightGBM] [Info] Number of data points in the train set: 832, number of used features: 40\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.868784\n",
      "failed with\n",
      "{'learning_rate': 0.34151260140761763, 'boosting_type': 'goss', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.5364782124240217, 'num_leaves': 11, 'min_data': 8, 'max_depth': 108}\n",
      "**************************************************\n",
      "Minimum is:  99999999999999999999999\n",
      "iteration number 18\n",
      "{'learning_rate': 0.07476209245611587, 'boosting_type': 'goss', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.36335586475005965, 'num_leaves': 1, 'min_data': 23, 'max_depth': 123} 5549\n",
      "failed with\n",
      "{'learning_rate': 0.07476209245611587, 'boosting_type': 'goss', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.36335586475005965, 'num_leaves': 1, 'min_data': 23, 'max_depth': 123}\n",
      "**************************************************\n",
      "Minimum is:  99999999999999999999999\n",
      "iteration number 19\n",
      "{'learning_rate': 0.1726001738082017, 'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.41257429077253116, 'num_leaves': 5, 'min_data': 17, 'max_depth': 8} 7927\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001391 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6839\n",
      "[LightGBM] [Info] Number of data points in the train set: 832, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 0.868784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed with\n",
      "{'learning_rate': 0.1726001738082017, 'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'mae', 'sub_feature': 0.41257429077253116, 'num_leaves': 5, 'min_data': 17, 'max_depth': 8}\n",
      "**************************************************\n",
      "Minimum is:  99999999999999999999999\n"
     ]
    }
   ],
   "source": [
    "#Set the minimum error arbitrarily large\n",
    "min = 99999999999999999999999 \n",
    "count = 0 #Used for keeping track of the iteration number\n",
    "#How many runs to perform using randomly selected hyperparameters\n",
    "iterations = 20\n",
    "for i in range(iterations):\n",
    "    print('iteration number', count)\n",
    "    count += 1 #increment count\n",
    "    try:\n",
    "        d_train = lgb.Dataset(X_train, label=y_train) #Load in data\n",
    "        params = {} #initialize parameters\n",
    "        params['learning_rate'] = np.random.uniform(0, 1)\n",
    "        params['boosting_type'] = np.random.choice(['gbdt', 'dart', 'goss'])\n",
    "        params['objective'] = 'regression'\n",
    "        params['metric'] = 'mae'\n",
    "        params['sub_feature'] = np.random.uniform(0, 1)\n",
    "        params['num_leaves'] = np.random.randint(1, 15)\n",
    "        params['min_data'] = np.random.randint(1, 30)\n",
    "        params['max_depth'] = np.random.randint(5, 200)\n",
    "        iterations = np.random.randint(10, 10000)\n",
    "        print(params, iterations)\n",
    "        #Train using selected parameters\n",
    "        clf = lgb.train(params, d_train, iterations)\n",
    "        y_pred=clf.predict(X_test) #Create predictions on test set\n",
    "        mae=mean_absolute_error(y_pred,y_test)\n",
    "        print('MAE:', mae)\n",
    "        if mae < min:\n",
    "            min = mae\n",
    "            pp = params \n",
    "    except: #in case something goes wrong\n",
    "        print('failed with')\n",
    "        print(params)\n",
    "    print(\"*\" * 50)\n",
    "    print('Minimum is: ', min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
