{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import tensorflow\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>Food Group</th>\n",
       "      <th>Price (£)</th>\n",
       "      <th>Weight (GRAMS)</th>\n",
       "      <th>Price per Weight (£/100Gram)</th>\n",
       "      <th>Carbon Group</th>\n",
       "      <th>Land use (m2/100g)</th>\n",
       "      <th>GHG(kgco2eq/100g)</th>\n",
       "      <th>Water use (L/100g)</th>\n",
       "      <th>Acidifying emissions(kgSO2eq per 100g)</th>\n",
       "      <th>...</th>\n",
       "      <th>Carotene, alpha (mcg)</th>\n",
       "      <th>Lycopene (mcg)</th>\n",
       "      <th>Lutein + Zeaxanthin (mcg)</th>\n",
       "      <th>Fatty acids, total monounsaturated (mg)</th>\n",
       "      <th>Fatty acids, total polyunsaturated (mg)</th>\n",
       "      <th>20:5 n-3 (EPA) (mg)</th>\n",
       "      <th>22:5 n-3 (DPA) (mg)</th>\n",
       "      <th>22:6 n-3 (DHA) (mg)</th>\n",
       "      <th>Caffeine (mg)</th>\n",
       "      <th>Theobromine (mg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Waffles Buttermilk Frozen Ready-To-Heat</td>\n",
       "      <td>Baked Foods</td>\n",
       "      <td>1.50</td>\n",
       "      <td>567.0</td>\n",
       "      <td>0.264550</td>\n",
       "      <td>Bread products</td>\n",
       "      <td>0.3482</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>4530.0</td>\n",
       "      <td>1445.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Waffle Buttermilk Frozen Ready-To-Heat Toasted</td>\n",
       "      <td>Baked Foods</td>\n",
       "      <td>1.50</td>\n",
       "      <td>567.0</td>\n",
       "      <td>0.264550</td>\n",
       "      <td>Bread products</td>\n",
       "      <td>0.3482</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>5292.0</td>\n",
       "      <td>1502.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Dutch Apple Pie</td>\n",
       "      <td>Baked Foods</td>\n",
       "      <td>2.80</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>Bread products</td>\n",
       "      <td>0.3482</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>5797.0</td>\n",
       "      <td>2117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Bread White Wheat</td>\n",
       "      <td>Baked Foods</td>\n",
       "      <td>0.95</td>\n",
       "      <td>800.0</td>\n",
       "      <td>0.118750</td>\n",
       "      <td>Bread products</td>\n",
       "      <td>0.3482</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>393.0</td>\n",
       "      <td>973.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Bagels Wheat</td>\n",
       "      <td>Baked Foods</td>\n",
       "      <td>1.60</td>\n",
       "      <td>450.0</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>Bread products</td>\n",
       "      <td>0.3482</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1218</td>\n",
       "      <td>Plantain Fried</td>\n",
       "      <td>Vegetables</td>\n",
       "      <td>0.90</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1.058824</td>\n",
       "      <td>Other vegetables</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>...</td>\n",
       "      <td>418.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4099.0</td>\n",
       "      <td>4079.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1219</td>\n",
       "      <td>Romaine Lettuce Raw</td>\n",
       "      <td>Vegetables</td>\n",
       "      <td>1.00</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>Other vegetables</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4204.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>Palak Paneer</td>\n",
       "      <td>Vegetables</td>\n",
       "      <td>3.75</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>Cheese</td>\n",
       "      <td>8.0642</td>\n",
       "      <td>2.1240</td>\n",
       "      <td>473.5</td>\n",
       "      <td>0.014894</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>4097.0</td>\n",
       "      <td>2402.0</td>\n",
       "      <td>2112.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1221</td>\n",
       "      <td>Carrots Raw Salad</td>\n",
       "      <td>Vegetables</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.041000</td>\n",
       "      <td>Other vegetables</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>...</td>\n",
       "      <td>2157.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>3498.0</td>\n",
       "      <td>9319.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1222</td>\n",
       "      <td>Corn on the cob</td>\n",
       "      <td>Vegetables</td>\n",
       "      <td>1.50</td>\n",
       "      <td>875.0</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>Other vegetables</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>693.0</td>\n",
       "      <td>373.0</td>\n",
       "      <td>518.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1042 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name   Food Group  Price (£)  \\\n",
       "1            Waffles Buttermilk Frozen Ready-To-Heat  Baked Foods       1.50   \n",
       "2     Waffle Buttermilk Frozen Ready-To-Heat Toasted  Baked Foods       1.50   \n",
       "5                                    Dutch Apple Pie  Baked Foods       2.80   \n",
       "9                                  Bread White Wheat  Baked Foods       0.95   \n",
       "10                                      Bagels Wheat  Baked Foods       1.60   \n",
       "...                                              ...          ...        ...   \n",
       "1218                                 Plantain Fried    Vegetables       0.90   \n",
       "1219                             Romaine Lettuce Raw   Vegetables       1.00   \n",
       "1220                                    Palak Paneer   Vegetables       3.75   \n",
       "1221                               Carrots Raw Salad   Vegetables       0.41   \n",
       "1222                                Corn on the cob    Vegetables       1.50   \n",
       "\n",
       "      Weight (GRAMS)  Price per Weight (£/100Gram)       Carbon Group  \\\n",
       "1              567.0                      0.264550    Bread products    \n",
       "2              567.0                      0.264550    Bread products    \n",
       "5              500.0                      0.560000    Bread products    \n",
       "9              800.0                      0.118750    Bread products    \n",
       "10             450.0                      0.355556    Bread products    \n",
       "...              ...                           ...                ...   \n",
       "1218            85.0                      1.058824  Other vegetables    \n",
       "1219           400.0                      0.250000  Other vegetables    \n",
       "1220           500.0                      0.750000            Cheese    \n",
       "1221          1000.0                      0.041000  Other vegetables    \n",
       "1222           875.0                      0.171429  Other vegetables    \n",
       "\n",
       "      Land use (m2/100g)  GHG(kgco2eq/100g)  Water use (L/100g)  \\\n",
       "1                 0.3482             0.1441                56.7   \n",
       "2                 0.3482             0.1441                56.7   \n",
       "5                 0.3482             0.1441                56.7   \n",
       "9                 0.3482             0.1441                56.7   \n",
       "10                0.3482             0.1441                56.7   \n",
       "...                  ...                ...                 ...   \n",
       "1218              0.0310             0.0455                 8.3   \n",
       "1219              0.0310             0.0455                 8.3   \n",
       "1220              8.0642             2.1240               473.5   \n",
       "1221              0.0310             0.0455                 8.3   \n",
       "1222              0.0310             0.0455                 8.3   \n",
       "\n",
       "      Acidifying emissions(kgSO2eq per 100g)  ...  Carotene, alpha (mcg)  \\\n",
       "1                                   0.001209  ...                    0.0   \n",
       "2                                   0.001209  ...                    0.0   \n",
       "5                                   0.001209  ...                    0.0   \n",
       "9                                   0.001209  ...                    0.0   \n",
       "10                                  0.001209  ...                    0.0   \n",
       "...                                      ...  ...                    ...   \n",
       "1218                                0.000531  ...                  418.0   \n",
       "1219                                0.000531  ...                    0.0   \n",
       "1220                                0.014894  ...                   12.0   \n",
       "1221                                0.000531  ...                 2157.0   \n",
       "1222                                0.000531  ...                    6.0   \n",
       "\n",
       "      Lycopene (mcg)  Lutein + Zeaxanthin (mcg)  \\\n",
       "1                0.0                       63.0   \n",
       "2                0.0                       66.0   \n",
       "5                1.0                       42.0   \n",
       "9                0.0                       25.0   \n",
       "10               0.0                       88.0   \n",
       "...              ...                        ...   \n",
       "1218             0.0                       29.0   \n",
       "1219             0.0                     4204.0   \n",
       "1220           313.0                     4097.0   \n",
       "1221             1.0                      162.0   \n",
       "1222             0.0                      693.0   \n",
       "\n",
       "      Fatty acids, total monounsaturated (mg)  \\\n",
       "1                                      4530.0   \n",
       "2                                      5292.0   \n",
       "5                                      5797.0   \n",
       "9                                       393.0   \n",
       "10                                      290.0   \n",
       "...                                       ...   \n",
       "1218                                   4099.0   \n",
       "1219                                      7.0   \n",
       "1220                                   2402.0   \n",
       "1221                                   3498.0   \n",
       "1222                                    373.0   \n",
       "\n",
       "      Fatty acids, total polyunsaturated (mg)  20:5 n-3 (EPA) (mg)  \\\n",
       "1                                      1445.0                 12.0   \n",
       "2                                      1502.0                 13.0   \n",
       "5                                      2117.0                  0.0   \n",
       "9                                       973.0                  3.0   \n",
       "10                                      936.0                  0.0   \n",
       "...                                       ...                  ...   \n",
       "1218                                   4079.0                  0.0   \n",
       "1219                                    126.0                  0.0   \n",
       "1220                                   2112.0                  0.0   \n",
       "1221                                   9319.0                  0.0   \n",
       "1222                                    518.0                  0.0   \n",
       "\n",
       "      22:5 n-3 (DPA) (mg)  22:6 n-3 (DHA) (mg)  Caffeine (mg)  \\\n",
       "1                     0.0                  7.0            0.0   \n",
       "2                     0.0                  8.0            0.0   \n",
       "5                     0.0                  0.0            0.0   \n",
       "9                     0.0                  0.0            0.0   \n",
       "10                    0.0                  0.0            0.0   \n",
       "...                   ...                  ...            ...   \n",
       "1218                  0.0                  0.0            0.0   \n",
       "1219                  0.0                  0.0            0.0   \n",
       "1220                  0.0                  0.0            0.0   \n",
       "1221                  0.0                  1.0            0.0   \n",
       "1222                  0.0                  0.0            0.0   \n",
       "\n",
       "      Theobromine (mg)  \n",
       "1                  0.0  \n",
       "2                  0.0  \n",
       "5                  0.0  \n",
       "9                  0.0  \n",
       "10                 0.0  \n",
       "...                ...  \n",
       "1218               0.0  \n",
       "1219               0.0  \n",
       "1220               0.0  \n",
       "1221               0.0  \n",
       "1222               0.0  \n",
       "\n",
       "[1042 rows x 51 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nutrition = pd.read_pickle(\"./Nutrition_Full_Features.pkl\")\n",
    "nutrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data for features\n",
    "X = nutrition.iloc[:, 11:]\n",
    "y = nutrition.iloc[:, 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Calories</th>\n",
       "      <th>Fat (g)</th>\n",
       "      <th>Protein (g)</th>\n",
       "      <th>Carbohydrate (g)</th>\n",
       "      <th>Sugars (g)</th>\n",
       "      <th>Fiber (g)</th>\n",
       "      <th>Saturated Fats (g)</th>\n",
       "      <th>Calcium (mg)</th>\n",
       "      <th>Iron, Fe (mg)</th>\n",
       "      <th>Potassium, K (mg)</th>\n",
       "      <th>...</th>\n",
       "      <th>Carotene, alpha (mcg)</th>\n",
       "      <th>Lycopene (mcg)</th>\n",
       "      <th>Lutein + Zeaxanthin (mcg)</th>\n",
       "      <th>Fatty acids, total monounsaturated (mg)</th>\n",
       "      <th>Fatty acids, total polyunsaturated (mg)</th>\n",
       "      <th>20:5 n-3 (EPA) (mg)</th>\n",
       "      <th>22:5 n-3 (DPA) (mg)</th>\n",
       "      <th>22:6 n-3 (DHA) (mg)</th>\n",
       "      <th>Caffeine (mg)</th>\n",
       "      <th>Theobromine (mg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>9.22</td>\n",
       "      <td>6.58</td>\n",
       "      <td>41.05</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.898</td>\n",
       "      <td>279</td>\n",
       "      <td>6.04</td>\n",
       "      <td>126.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>4530.0</td>\n",
       "      <td>1445.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>309</td>\n",
       "      <td>9.49</td>\n",
       "      <td>7.42</td>\n",
       "      <td>48.39</td>\n",
       "      <td>4.41</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.275</td>\n",
       "      <td>299</td>\n",
       "      <td>6.59</td>\n",
       "      <td>138.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>5292.0</td>\n",
       "      <td>1502.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>290</td>\n",
       "      <td>11.50</td>\n",
       "      <td>2.17</td>\n",
       "      <td>44.54</td>\n",
       "      <td>22.02</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.313</td>\n",
       "      <td>14</td>\n",
       "      <td>0.91</td>\n",
       "      <td>76.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>5797.0</td>\n",
       "      <td>2117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>238</td>\n",
       "      <td>2.15</td>\n",
       "      <td>10.66</td>\n",
       "      <td>43.91</td>\n",
       "      <td>5.00</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.630</td>\n",
       "      <td>684</td>\n",
       "      <td>4.89</td>\n",
       "      <td>127.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>393.0</td>\n",
       "      <td>973.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>250</td>\n",
       "      <td>1.53</td>\n",
       "      <td>10.20</td>\n",
       "      <td>48.89</td>\n",
       "      <td>6.12</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>20</td>\n",
       "      <td>2.76</td>\n",
       "      <td>165.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1218</td>\n",
       "      <td>241</td>\n",
       "      <td>10.16</td>\n",
       "      <td>1.66</td>\n",
       "      <td>40.60</td>\n",
       "      <td>19.10</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.507</td>\n",
       "      <td>4</td>\n",
       "      <td>0.78</td>\n",
       "      <td>572.0</td>\n",
       "      <td>...</td>\n",
       "      <td>418.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4099.0</td>\n",
       "      <td>4079.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1219</td>\n",
       "      <td>19</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.39</td>\n",
       "      <td>3.78</td>\n",
       "      <td>0.71</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.053</td>\n",
       "      <td>62</td>\n",
       "      <td>0.90</td>\n",
       "      <td>327.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4204.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>96</td>\n",
       "      <td>6.84</td>\n",
       "      <td>5.23</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.89</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.486</td>\n",
       "      <td>70</td>\n",
       "      <td>1.15</td>\n",
       "      <td>269.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>4097.0</td>\n",
       "      <td>2402.0</td>\n",
       "      <td>2112.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1221</td>\n",
       "      <td>208</td>\n",
       "      <td>15.70</td>\n",
       "      <td>1.22</td>\n",
       "      <td>17.17</td>\n",
       "      <td>11.23</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.452</td>\n",
       "      <td>30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>309.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2157.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>3498.0</td>\n",
       "      <td>9319.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1222</td>\n",
       "      <td>67</td>\n",
       "      <td>1.22</td>\n",
       "      <td>2.28</td>\n",
       "      <td>14.30</td>\n",
       "      <td>4.43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.244</td>\n",
       "      <td>3</td>\n",
       "      <td>0.27</td>\n",
       "      <td>132.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>693.0</td>\n",
       "      <td>373.0</td>\n",
       "      <td>518.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1042 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Calories  Fat (g)  Protein (g)  Carbohydrate (g)  Sugars (g)  Fiber (g)  \\\n",
       "1          273     9.22         6.58             41.05        4.30        2.2   \n",
       "2          309     9.49         7.42             48.39        4.41        2.6   \n",
       "5          290    11.50         2.17             44.54       22.02        1.6   \n",
       "9          238     2.15        10.66             43.91        5.00        9.2   \n",
       "10         250     1.53        10.20             48.89        6.12        4.1   \n",
       "...        ...      ...          ...               ...         ...        ...   \n",
       "1218       241    10.16         1.66             40.60       19.10        2.9   \n",
       "1219        19     0.27         1.39              3.78        0.71        3.1   \n",
       "1220        96     6.84         5.23              4.32        1.89        1.2   \n",
       "1221       208    15.70         1.22             17.17       11.23        2.3   \n",
       "1222        67     1.22         2.28             14.30        4.43        2.0   \n",
       "\n",
       "      Saturated Fats (g)  Calcium (mg)  Iron, Fe (mg)  Potassium, K (mg)  ...  \\\n",
       "1                  1.898           279           6.04              126.0  ...   \n",
       "2                  2.275           299           6.59              138.0  ...   \n",
       "5                  2.313            14           0.91               76.0  ...   \n",
       "9                  0.630           684           4.89              127.0  ...   \n",
       "10                 0.000            20           2.76              165.0  ...   \n",
       "...                  ...           ...            ...                ...  ...   \n",
       "1218               1.507             4           0.78              572.0  ...   \n",
       "1219               0.053            62           0.90              327.0  ...   \n",
       "1220               1.486            70           1.15              269.0  ...   \n",
       "1221               2.452            30           0.49              309.0  ...   \n",
       "1222               0.244             3           0.27              132.0  ...   \n",
       "\n",
       "      Carotene, alpha (mcg)  Lycopene (mcg)  Lutein + Zeaxanthin (mcg)  \\\n",
       "1                       0.0             0.0                       63.0   \n",
       "2                       0.0             0.0                       66.0   \n",
       "5                       0.0             1.0                       42.0   \n",
       "9                       0.0             0.0                       25.0   \n",
       "10                      0.0             0.0                       88.0   \n",
       "...                     ...             ...                        ...   \n",
       "1218                  418.0             0.0                       29.0   \n",
       "1219                    0.0             0.0                     4204.0   \n",
       "1220                   12.0           313.0                     4097.0   \n",
       "1221                 2157.0             1.0                      162.0   \n",
       "1222                    6.0             0.0                      693.0   \n",
       "\n",
       "      Fatty acids, total monounsaturated (mg)  \\\n",
       "1                                      4530.0   \n",
       "2                                      5292.0   \n",
       "5                                      5797.0   \n",
       "9                                       393.0   \n",
       "10                                      290.0   \n",
       "...                                       ...   \n",
       "1218                                   4099.0   \n",
       "1219                                      7.0   \n",
       "1220                                   2402.0   \n",
       "1221                                   3498.0   \n",
       "1222                                    373.0   \n",
       "\n",
       "      Fatty acids, total polyunsaturated (mg)  20:5 n-3 (EPA) (mg)  \\\n",
       "1                                      1445.0                 12.0   \n",
       "2                                      1502.0                 13.0   \n",
       "5                                      2117.0                  0.0   \n",
       "9                                       973.0                  3.0   \n",
       "10                                      936.0                  0.0   \n",
       "...                                       ...                  ...   \n",
       "1218                                   4079.0                  0.0   \n",
       "1219                                    126.0                  0.0   \n",
       "1220                                   2112.0                  0.0   \n",
       "1221                                   9319.0                  0.0   \n",
       "1222                                    518.0                  0.0   \n",
       "\n",
       "      22:5 n-3 (DPA) (mg)  22:6 n-3 (DHA) (mg)  Caffeine (mg)  \\\n",
       "1                     0.0                  7.0            0.0   \n",
       "2                     0.0                  8.0            0.0   \n",
       "5                     0.0                  0.0            0.0   \n",
       "9                     0.0                  0.0            0.0   \n",
       "10                    0.0                  0.0            0.0   \n",
       "...                   ...                  ...            ...   \n",
       "1218                  0.0                  0.0            0.0   \n",
       "1219                  0.0                  0.0            0.0   \n",
       "1220                  0.0                  0.0            0.0   \n",
       "1221                  0.0                  1.0            0.0   \n",
       "1222                  0.0                  0.0            0.0   \n",
       "\n",
       "      Theobromine (mg)  \n",
       "1                  0.0  \n",
       "2                  0.0  \n",
       "5                  0.0  \n",
       "9                  0.0  \n",
       "10                 0.0  \n",
       "...                ...  \n",
       "1218               0.0  \n",
       "1219               0.0  \n",
       "1220               0.0  \n",
       "1221               0.0  \n",
       "1222               0.0  \n",
       "\n",
       "[1042 rows x 40 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       0.264550\n",
       "2       0.264550\n",
       "5       0.560000\n",
       "9       0.118750\n",
       "10      0.355556\n",
       "          ...   \n",
       "1218    1.058824\n",
       "1219    0.250000\n",
       "1220    0.750000\n",
       "1221    0.041000\n",
       "1222    0.171429\n",
       "Name: Price per Weight (£/100Gram), Length: 1042, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# command/\n",
    "#only normalising X not y. Is this right?\n",
    "# norm_X = preprocessing.normalize(X, axis=0) #collum instead of row\n",
    "# norm_X = pd.DataFrame(norm_X, columns = X.columns)\n",
    "# X=norm_X\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.ravel(y)\n",
    "X = X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "84/84 [==============================] - 0s 629us/step - loss: 15.9264 - mse: 15.9264\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 0s 636us/step - loss: 19.9315 - mse: 19.9315\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 0s 649us/step - loss: 5.7282 - mse: 5.7282\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 0s 906us/step - loss: 165.0229 - mse: 165.0229\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 0s 977us/step - loss: 79.3721 - mse: 79.3721\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 0s 759us/step - loss: 35.9677 - mse: 35.9677\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 0s 659us/step - loss: 59.4584 - mse: 59.4584\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 0s 651us/step - loss: 2.8918 - mse: 2.8918\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 0s 574us/step - loss: 7.6249 - mse: 7.6249\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 0s 582us/step - loss: 139.5200 - mse: 139.5200\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 0s 612us/step - loss: 57.7804 - mse: 57.7804\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 0s 625us/step - loss: 38.3268 - mse: 38.3268\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 0s 638us/step - loss: 124.2218 - mse: 124.2218\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 0s 637us/step - loss: 163.5450 - mse: 163.5450\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 0s 693us/step - loss: 30.7409 - mse: 30.7409\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 0s 631us/step - loss: 105.2233 - mse: 105.2233\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 0s 633us/step - loss: 23.2306 - mse: 23.2306\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 0s 729us/step - loss: 30.0722 - mse: 30.0722\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 0s 595us/step - loss: 22.8951 - mse: 22.8951\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 0s 644us/step - loss: 36.9539 - mse: 36.9539\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 0s 614us/step - loss: 40.7229 - mse: 40.7229\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 0s 580us/step - loss: 3.4453 - mse: 3.4453\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 0s 584us/step - loss: 16.5116 - mse: 16.5116\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 0s 641us/step - loss: 17.3766 - mse: 17.3766\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 0s 634us/step - loss: 50.9344 - mse: 50.9344\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 0s 627us/step - loss: 84.1814 - mse: 84.1814\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 0s 629us/step - loss: 70.5488 - mse: 70.5488\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 0s 603us/step - loss: 57.2834 - mse: 57.2834\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 0s 600us/step - loss: 78.7802 - mse: 78.7802\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 0s 600us/step - loss: 84.3871 - mse: 84.3871\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 0s 600us/step - loss: 67.2036 - mse: 67.2036\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 0s 591us/step - loss: 64.2622 - mse: 64.2622\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 0s 598us/step - loss: 117.2659 - mse: 117.2659\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 0s 579us/step - loss: 54.6451 - mse: 54.6451\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 0s 593us/step - loss: 90.1794 - mse: 90.1794\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 0s 585us/step - loss: 170.9406 - mse: 170.9406\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 0s 625us/step - loss: 46.4594 - mse: 46.4594\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 0s 633us/step - loss: 97.4811 - mse: 97.4811\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 0s 633us/step - loss: 29.6481 - mse: 29.6481\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 0s 632us/step - loss: 25.6132 - mse: 25.6132\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 0s 583us/step - loss: 156.3605 - mse: 156.3605\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 0s 587us/step - loss: 6.7129 - mse: 6.7129\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 0s 575us/step - loss: 20.0669 - mse: 20.0669\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 0s 582us/step - loss: 33.9490 - mse: 33.9490\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 0s 593us/step - loss: 59.3263 - mse: 59.3263\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 0s 596us/step - loss: 41.5963 - mse: 41.5963\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 0s 652us/step - loss: 104.9946 - mse: 104.9946\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 0s 666us/step - loss: 45.6481 - mse: 45.6481\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 0s 659us/step - loss: 14.5837 - mse: 14.5837\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 0s 666us/step - loss: 14.1607 - mse: 14.1607\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 0s 646us/step - loss: 83.1148 - mse: 83.1148\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 0s 597us/step - loss: 78.3645 - mse: 78.3645\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 0s 603us/step - loss: 105.0263 - mse: 105.0263\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 0s 562us/step - loss: 109.1010 - mse: 109.1010\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 0s 573us/step - loss: 42.7910 - mse: 42.7910\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 0s 583us/step - loss: 6.0278 - mse: 6.0278\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 0s 598us/step - loss: 32.3129 - mse: 32.3129\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 0s 729us/step - loss: 32.2753 - mse: 32.2753\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 0s 624us/step - loss: 81.5606 - mse: 81.5606\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 0s 601us/step - loss: 24.1909 - mse: 24.1909\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 0s 612us/step - loss: 123.2362 - mse: 123.2362\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 0s 553us/step - loss: 72.8060 - mse: 72.8060\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 0s 551us/step - loss: 146.0987 - mse: 146.0987\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 0s 562us/step - loss: 57.8472 - mse: 57.8472\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 0s 563us/step - loss: 76.1982 - mse: 76.1982\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 0s 601us/step - loss: 43.3107 - mse: 43.3107\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 0s 595us/step - loss: 50.2954 - mse: 50.2954\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 0s 584us/step - loss: 25.0652 - mse: 25.0652\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 0s 585us/step - loss: 89.9817 - mse: 89.9817\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 0s 586us/step - loss: 32.0154 - mse: 32.0154\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 0s 586us/step - loss: 17.5705 - mse: 17.5705\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 0s 584us/step - loss: 68.6487 - mse: 68.6487\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 0s 837us/step - loss: 91.5862 - mse: 91.5862\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 0s 700us/step - loss: 47.7342 - mse: 47.7342\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 0s 587us/step - loss: 81.9764 - mse: 81.9764\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 0s 559us/step - loss: 91.3123 - mse: 91.3123\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 0s 572us/step - loss: 71.5799 - mse: 71.5799\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 0s 565us/step - loss: 37.7826 - mse: 37.7826\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 0s 622us/step - loss: 22.6674 - mse: 22.6674\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 0s 618us/step - loss: 17.9556 - mse: 17.9556\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 0s 616us/step - loss: 15.7968 - mse: 15.7968\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 0s 617us/step - loss: 58.9107 - mse: 58.9107\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 0s 627us/step - loss: 18.8657 - mse: 18.8657\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 0s 623us/step - loss: 65.0631 - mse: 65.0631\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 0s 587us/step - loss: 34.9923 - mse: 34.9923\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 0s 587us/step - loss: 8.7789 - mse: 8.7789\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 0s 568us/step - loss: 38.1254 - mse: 38.1254\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 0s 587us/step - loss: 44.6794 - mse: 44.6794\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 0s 577us/step - loss: 5.5873 - mse: 5.5873\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 0s 592us/step - loss: 13.0920 - mse: 13.0920\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 0s 618us/step - loss: 11.6483 - mse: 11.6483\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 0s 597us/step - loss: 18.0330 - mse: 18.0330\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 0s 622us/step - loss: 3.6432 - mse: 3.6432\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 0s 584us/step - loss: 48.7851 - mse: 48.7851\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 0s 563us/step - loss: 18.7411 - mse: 18.7411\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 0s 567us/step - loss: 62.4583 - mse: 62.4583\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 0s 568us/step - loss: 51.1521 - mse: 51.1521\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 0s 553us/step - loss: 43.7283 - mse: 43.7283\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 0s 555us/step - loss: 2.9043 - mse: 2.9043\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 0s 549us/step - loss: 29.3404 - mse: 29.3404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdee35e82d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Feature Scaling/preprocessing - normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Building model\n",
    "model = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "model.add(Dense(8, activation = 'relu', input_dim = 40))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "model.add(Dense(units = 16, activation = 'relu'))\n",
    "\n",
    "#avoids overfitting\n",
    "#https://keras.io/api/layers/regularization_layers/dropout/\n",
    "model.add(keras.layers.Dropout(0.1))\n",
    "\n",
    "\n",
    "# Adding the output layer\n",
    "\n",
    "model.add(Dense(units = 1))\n",
    "\n",
    "\n",
    "# Compiling the ANN\n",
    "model.compile(optimizer = 'adam', loss = 'mse', metrics=['mse'])#=history\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "model.fit(X_train, y_train, batch_size = 10, epochs = 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters involved:\n",
    "    https://keras.io/api/models/model_training_apis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 1ms/step - loss: 15.3288 - mse: 15.3288\n"
     ]
    }
   ],
   "source": [
    "#model evaluation\n",
    "mse = model.evaluate(X_test, y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE without tuning: 15.328757286071777\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('MSE without tuning: {}'.format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 8)                 328       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 489\n",
      "Trainable params: 489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/api/models/sequential/\n",
    "The Sequential Class, provides a training and inference features on this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "plt.plot(y_test, color = 'red', label = 'Real data')\n",
    "plt.plot(y_pred, color = 'blue', label = 'Predicted data')\n",
    "plt.title('Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cite: https://stackoverflow.com/questions/49008074/how-to-create-a-neural-network-for-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 1.2121121408477675\n",
      "Mean Squared Error: 15.328759903773157\n",
      "Root Mean Squared Error: 3.915196023671504\n"
     ]
    }
   ],
   "source": [
    "#Why are my errors all of a sudden so because of overfitting?\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kerastuner\n",
    "from kerastuner import HyperModel\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('units', min_value=8, max_value=64, step=4), \n",
    "        activation = hp.Choice('dense_activation', \n",
    "                values=['relu', 'tanh','sigmoid','softmax']),\n",
    "        input_dim = 40\n",
    "        )\n",
    "             \n",
    "    )\n",
    "\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('units', min_value=8, max_value=64, step=4), \n",
    "        activation = hp.Choice('dense_activation', \n",
    "                values=['relu', 'tanh','sigmoid','softmax'])\n",
    "        )\n",
    "             \n",
    "    )\n",
    "    \n",
    "    model.add(\n",
    "        keras.layers.Dropout(\n",
    "            hp.Float(\n",
    "                    'dropout',\n",
    "                    min_value=0.0,\n",
    "                    max_value=0.1,\n",
    "                    step=0.01)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Adding the output layer\n",
    "    model.add(Dense(units = 1))\n",
    "\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = hp.Choice('dense_optimizer',\n",
    "                values=['adam','SGD','rmsprop','adadelta'] ),\n",
    "        loss = 'mse',\n",
    "        metrics = ['mse']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_rs = kerastuner.tuners.RandomSearch(\n",
    "            build_model,\n",
    "            objective='mse',\n",
    "            max_trials=20,\n",
    "            executions_per_trial=2, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 00m 01s]\n",
      "mse: 63.7579345703125\n",
      "\n",
      "Best mse So Far: 61.91103935241699\n",
      "Total elapsed time: 00h 00m 30s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner_rs.search(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Results summary\n",
      "Results in ./untitled_project\n",
      "Showing 10 best trials\n",
      "Objective(name='mse', direction='min')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 28\n",
      "dense_activation: relu\n",
      "dropout: 0.02\n",
      "dense_optimizer: adam\n",
      "Score: 62.04389572143555\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 24\n",
      "dense_activation: relu\n",
      "dropout: 0.08\n",
      "dense_optimizer: adam\n",
      "Score: 62.12020492553711\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 48\n",
      "dense_activation: tanh\n",
      "dropout: 0.0\n",
      "dense_optimizer: rmsprop\n",
      "Score: 62.342777252197266\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 48\n",
      "dense_activation: sigmoid\n",
      "dropout: 0.08\n",
      "dense_optimizer: adam\n",
      "Score: 62.80833435058594\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 60\n",
      "dense_activation: sigmoid\n",
      "dropout: 0.06\n",
      "dense_optimizer: rmsprop\n",
      "Score: 62.891733169555664\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 48\n",
      "dense_activation: softmax\n",
      "dropout: 0.05\n",
      "dense_optimizer: SGD\n",
      "Score: 63.199811935424805\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 52\n",
      "dense_activation: softmax\n",
      "dropout: 0.05\n",
      "dense_optimizer: SGD\n",
      "Score: 63.210819244384766\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 44\n",
      "dense_activation: relu\n",
      "dropout: 0.04\n",
      "dense_optimizer: adadelta\n",
      "Score: 64.16831398010254\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 8\n",
      "dense_activation: tanh\n",
      "dropout: 0.08\n",
      "dense_optimizer: adadelta\n",
      "Score: 64.47218704223633\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 12\n",
      "dense_activation: tanh\n",
      "dropout: 0.04\n",
      "dense_optimizer: adadelta\n",
      "Score: 64.87764739990234\n"
     ]
    }
   ],
   "source": [
    "best_model = tuner_rs.get_best_models(num_models=2)\n",
    "#loss, mse = best_model.evaluate(X_test, y_test)\n",
    "tuner_rs.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
