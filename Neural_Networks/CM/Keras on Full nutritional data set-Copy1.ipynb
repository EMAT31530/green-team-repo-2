{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import tensorflow\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>Food Group</th>\n",
       "      <th>Price (£)</th>\n",
       "      <th>Weight (GRAMS)</th>\n",
       "      <th>Price per Weight (£/100Gram)</th>\n",
       "      <th>Carbon Group</th>\n",
       "      <th>Land use (m2/100g)</th>\n",
       "      <th>GHG(kgco2eq/100g)</th>\n",
       "      <th>Water use (L/100g)</th>\n",
       "      <th>Acidifying emissions(kgSO2eq per 100g)</th>\n",
       "      <th>...</th>\n",
       "      <th>Carotene, alpha (mcg)</th>\n",
       "      <th>Lycopene (mcg)</th>\n",
       "      <th>Lutein + Zeaxanthin (mcg)</th>\n",
       "      <th>Fatty acids, total monounsaturated (mg)</th>\n",
       "      <th>Fatty acids, total polyunsaturated (mg)</th>\n",
       "      <th>20:5 n-3 (EPA) (mg)</th>\n",
       "      <th>22:5 n-3 (DPA) (mg)</th>\n",
       "      <th>22:6 n-3 (DHA) (mg)</th>\n",
       "      <th>Caffeine (mg)</th>\n",
       "      <th>Theobromine (mg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Waffles Buttermilk Frozen Ready-To-Heat</td>\n",
       "      <td>Baked Foods</td>\n",
       "      <td>1.50</td>\n",
       "      <td>567.0</td>\n",
       "      <td>0.264550</td>\n",
       "      <td>Bread products</td>\n",
       "      <td>0.3482</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>4530.0</td>\n",
       "      <td>1445.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Waffle Buttermilk Frozen Ready-To-Heat Toasted</td>\n",
       "      <td>Baked Foods</td>\n",
       "      <td>1.50</td>\n",
       "      <td>567.0</td>\n",
       "      <td>0.264550</td>\n",
       "      <td>Bread products</td>\n",
       "      <td>0.3482</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>5292.0</td>\n",
       "      <td>1502.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Dutch Apple Pie</td>\n",
       "      <td>Baked Foods</td>\n",
       "      <td>2.80</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>Bread products</td>\n",
       "      <td>0.3482</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>5797.0</td>\n",
       "      <td>2117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Bread White Wheat</td>\n",
       "      <td>Baked Foods</td>\n",
       "      <td>0.95</td>\n",
       "      <td>800.0</td>\n",
       "      <td>0.118750</td>\n",
       "      <td>Bread products</td>\n",
       "      <td>0.3482</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>393.0</td>\n",
       "      <td>973.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Bagels Wheat</td>\n",
       "      <td>Baked Foods</td>\n",
       "      <td>1.60</td>\n",
       "      <td>450.0</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>Bread products</td>\n",
       "      <td>0.3482</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1218</td>\n",
       "      <td>Plantain Fried</td>\n",
       "      <td>Vegetables</td>\n",
       "      <td>0.90</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1.058824</td>\n",
       "      <td>Other vegetables</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>...</td>\n",
       "      <td>418.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4099.0</td>\n",
       "      <td>4079.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1219</td>\n",
       "      <td>Romaine Lettuce Raw</td>\n",
       "      <td>Vegetables</td>\n",
       "      <td>1.00</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>Other vegetables</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4204.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>Palak Paneer</td>\n",
       "      <td>Vegetables</td>\n",
       "      <td>3.75</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>Cheese</td>\n",
       "      <td>8.0642</td>\n",
       "      <td>2.1240</td>\n",
       "      <td>473.5</td>\n",
       "      <td>0.014894</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>4097.0</td>\n",
       "      <td>2402.0</td>\n",
       "      <td>2112.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1221</td>\n",
       "      <td>Carrots Raw Salad</td>\n",
       "      <td>Vegetables</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.041000</td>\n",
       "      <td>Other vegetables</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>...</td>\n",
       "      <td>2157.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>3498.0</td>\n",
       "      <td>9319.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1222</td>\n",
       "      <td>Corn on the cob</td>\n",
       "      <td>Vegetables</td>\n",
       "      <td>1.50</td>\n",
       "      <td>875.0</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>Other vegetables</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>693.0</td>\n",
       "      <td>373.0</td>\n",
       "      <td>518.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1042 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name   Food Group  Price (£)  \\\n",
       "1            Waffles Buttermilk Frozen Ready-To-Heat  Baked Foods       1.50   \n",
       "2     Waffle Buttermilk Frozen Ready-To-Heat Toasted  Baked Foods       1.50   \n",
       "5                                    Dutch Apple Pie  Baked Foods       2.80   \n",
       "9                                  Bread White Wheat  Baked Foods       0.95   \n",
       "10                                      Bagels Wheat  Baked Foods       1.60   \n",
       "...                                              ...          ...        ...   \n",
       "1218                                 Plantain Fried    Vegetables       0.90   \n",
       "1219                             Romaine Lettuce Raw   Vegetables       1.00   \n",
       "1220                                    Palak Paneer   Vegetables       3.75   \n",
       "1221                               Carrots Raw Salad   Vegetables       0.41   \n",
       "1222                                Corn on the cob    Vegetables       1.50   \n",
       "\n",
       "      Weight (GRAMS)  Price per Weight (£/100Gram)       Carbon Group  \\\n",
       "1              567.0                      0.264550    Bread products    \n",
       "2              567.0                      0.264550    Bread products    \n",
       "5              500.0                      0.560000    Bread products    \n",
       "9              800.0                      0.118750    Bread products    \n",
       "10             450.0                      0.355556    Bread products    \n",
       "...              ...                           ...                ...   \n",
       "1218            85.0                      1.058824  Other vegetables    \n",
       "1219           400.0                      0.250000  Other vegetables    \n",
       "1220           500.0                      0.750000            Cheese    \n",
       "1221          1000.0                      0.041000  Other vegetables    \n",
       "1222           875.0                      0.171429  Other vegetables    \n",
       "\n",
       "      Land use (m2/100g)  GHG(kgco2eq/100g)  Water use (L/100g)  \\\n",
       "1                 0.3482             0.1441                56.7   \n",
       "2                 0.3482             0.1441                56.7   \n",
       "5                 0.3482             0.1441                56.7   \n",
       "9                 0.3482             0.1441                56.7   \n",
       "10                0.3482             0.1441                56.7   \n",
       "...                  ...                ...                 ...   \n",
       "1218              0.0310             0.0455                 8.3   \n",
       "1219              0.0310             0.0455                 8.3   \n",
       "1220              8.0642             2.1240               473.5   \n",
       "1221              0.0310             0.0455                 8.3   \n",
       "1222              0.0310             0.0455                 8.3   \n",
       "\n",
       "      Acidifying emissions(kgSO2eq per 100g)  ...  Carotene, alpha (mcg)  \\\n",
       "1                                   0.001209  ...                    0.0   \n",
       "2                                   0.001209  ...                    0.0   \n",
       "5                                   0.001209  ...                    0.0   \n",
       "9                                   0.001209  ...                    0.0   \n",
       "10                                  0.001209  ...                    0.0   \n",
       "...                                      ...  ...                    ...   \n",
       "1218                                0.000531  ...                  418.0   \n",
       "1219                                0.000531  ...                    0.0   \n",
       "1220                                0.014894  ...                   12.0   \n",
       "1221                                0.000531  ...                 2157.0   \n",
       "1222                                0.000531  ...                    6.0   \n",
       "\n",
       "      Lycopene (mcg)  Lutein + Zeaxanthin (mcg)  \\\n",
       "1                0.0                       63.0   \n",
       "2                0.0                       66.0   \n",
       "5                1.0                       42.0   \n",
       "9                0.0                       25.0   \n",
       "10               0.0                       88.0   \n",
       "...              ...                        ...   \n",
       "1218             0.0                       29.0   \n",
       "1219             0.0                     4204.0   \n",
       "1220           313.0                     4097.0   \n",
       "1221             1.0                      162.0   \n",
       "1222             0.0                      693.0   \n",
       "\n",
       "      Fatty acids, total monounsaturated (mg)  \\\n",
       "1                                      4530.0   \n",
       "2                                      5292.0   \n",
       "5                                      5797.0   \n",
       "9                                       393.0   \n",
       "10                                      290.0   \n",
       "...                                       ...   \n",
       "1218                                   4099.0   \n",
       "1219                                      7.0   \n",
       "1220                                   2402.0   \n",
       "1221                                   3498.0   \n",
       "1222                                    373.0   \n",
       "\n",
       "      Fatty acids, total polyunsaturated (mg)  20:5 n-3 (EPA) (mg)  \\\n",
       "1                                      1445.0                 12.0   \n",
       "2                                      1502.0                 13.0   \n",
       "5                                      2117.0                  0.0   \n",
       "9                                       973.0                  3.0   \n",
       "10                                      936.0                  0.0   \n",
       "...                                       ...                  ...   \n",
       "1218                                   4079.0                  0.0   \n",
       "1219                                    126.0                  0.0   \n",
       "1220                                   2112.0                  0.0   \n",
       "1221                                   9319.0                  0.0   \n",
       "1222                                    518.0                  0.0   \n",
       "\n",
       "      22:5 n-3 (DPA) (mg)  22:6 n-3 (DHA) (mg)  Caffeine (mg)  \\\n",
       "1                     0.0                  7.0            0.0   \n",
       "2                     0.0                  8.0            0.0   \n",
       "5                     0.0                  0.0            0.0   \n",
       "9                     0.0                  0.0            0.0   \n",
       "10                    0.0                  0.0            0.0   \n",
       "...                   ...                  ...            ...   \n",
       "1218                  0.0                  0.0            0.0   \n",
       "1219                  0.0                  0.0            0.0   \n",
       "1220                  0.0                  0.0            0.0   \n",
       "1221                  0.0                  1.0            0.0   \n",
       "1222                  0.0                  0.0            0.0   \n",
       "\n",
       "      Theobromine (mg)  \n",
       "1                  0.0  \n",
       "2                  0.0  \n",
       "5                  0.0  \n",
       "9                  0.0  \n",
       "10                 0.0  \n",
       "...                ...  \n",
       "1218               0.0  \n",
       "1219               0.0  \n",
       "1220               0.0  \n",
       "1221               0.0  \n",
       "1222               0.0  \n",
       "\n",
       "[1042 rows x 51 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nutrition = pd.read_pickle(\"./Nutrition_Full_Features.pkl\")\n",
    "nutrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data for features\n",
    "X = nutrition.iloc[:, 11:]\n",
    "y = nutrition.iloc[:, 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Calories</th>\n",
       "      <th>Fat (g)</th>\n",
       "      <th>Protein (g)</th>\n",
       "      <th>Carbohydrate (g)</th>\n",
       "      <th>Sugars (g)</th>\n",
       "      <th>Fiber (g)</th>\n",
       "      <th>Saturated Fats (g)</th>\n",
       "      <th>Calcium (mg)</th>\n",
       "      <th>Iron, Fe (mg)</th>\n",
       "      <th>Potassium, K (mg)</th>\n",
       "      <th>...</th>\n",
       "      <th>Carotene, alpha (mcg)</th>\n",
       "      <th>Lycopene (mcg)</th>\n",
       "      <th>Lutein + Zeaxanthin (mcg)</th>\n",
       "      <th>Fatty acids, total monounsaturated (mg)</th>\n",
       "      <th>Fatty acids, total polyunsaturated (mg)</th>\n",
       "      <th>20:5 n-3 (EPA) (mg)</th>\n",
       "      <th>22:5 n-3 (DPA) (mg)</th>\n",
       "      <th>22:6 n-3 (DHA) (mg)</th>\n",
       "      <th>Caffeine (mg)</th>\n",
       "      <th>Theobromine (mg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>9.22</td>\n",
       "      <td>6.58</td>\n",
       "      <td>41.05</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.898</td>\n",
       "      <td>279</td>\n",
       "      <td>6.04</td>\n",
       "      <td>126.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>4530.0</td>\n",
       "      <td>1445.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>309</td>\n",
       "      <td>9.49</td>\n",
       "      <td>7.42</td>\n",
       "      <td>48.39</td>\n",
       "      <td>4.41</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.275</td>\n",
       "      <td>299</td>\n",
       "      <td>6.59</td>\n",
       "      <td>138.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>5292.0</td>\n",
       "      <td>1502.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>290</td>\n",
       "      <td>11.50</td>\n",
       "      <td>2.17</td>\n",
       "      <td>44.54</td>\n",
       "      <td>22.02</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.313</td>\n",
       "      <td>14</td>\n",
       "      <td>0.91</td>\n",
       "      <td>76.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>5797.0</td>\n",
       "      <td>2117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>238</td>\n",
       "      <td>2.15</td>\n",
       "      <td>10.66</td>\n",
       "      <td>43.91</td>\n",
       "      <td>5.00</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.630</td>\n",
       "      <td>684</td>\n",
       "      <td>4.89</td>\n",
       "      <td>127.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>393.0</td>\n",
       "      <td>973.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>250</td>\n",
       "      <td>1.53</td>\n",
       "      <td>10.20</td>\n",
       "      <td>48.89</td>\n",
       "      <td>6.12</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>20</td>\n",
       "      <td>2.76</td>\n",
       "      <td>165.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1218</td>\n",
       "      <td>241</td>\n",
       "      <td>10.16</td>\n",
       "      <td>1.66</td>\n",
       "      <td>40.60</td>\n",
       "      <td>19.10</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.507</td>\n",
       "      <td>4</td>\n",
       "      <td>0.78</td>\n",
       "      <td>572.0</td>\n",
       "      <td>...</td>\n",
       "      <td>418.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4099.0</td>\n",
       "      <td>4079.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1219</td>\n",
       "      <td>19</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.39</td>\n",
       "      <td>3.78</td>\n",
       "      <td>0.71</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.053</td>\n",
       "      <td>62</td>\n",
       "      <td>0.90</td>\n",
       "      <td>327.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4204.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>96</td>\n",
       "      <td>6.84</td>\n",
       "      <td>5.23</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.89</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.486</td>\n",
       "      <td>70</td>\n",
       "      <td>1.15</td>\n",
       "      <td>269.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>4097.0</td>\n",
       "      <td>2402.0</td>\n",
       "      <td>2112.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1221</td>\n",
       "      <td>208</td>\n",
       "      <td>15.70</td>\n",
       "      <td>1.22</td>\n",
       "      <td>17.17</td>\n",
       "      <td>11.23</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.452</td>\n",
       "      <td>30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>309.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2157.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>3498.0</td>\n",
       "      <td>9319.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1222</td>\n",
       "      <td>67</td>\n",
       "      <td>1.22</td>\n",
       "      <td>2.28</td>\n",
       "      <td>14.30</td>\n",
       "      <td>4.43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.244</td>\n",
       "      <td>3</td>\n",
       "      <td>0.27</td>\n",
       "      <td>132.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>693.0</td>\n",
       "      <td>373.0</td>\n",
       "      <td>518.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1042 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Calories  Fat (g)  Protein (g)  Carbohydrate (g)  Sugars (g)  Fiber (g)  \\\n",
       "1          273     9.22         6.58             41.05        4.30        2.2   \n",
       "2          309     9.49         7.42             48.39        4.41        2.6   \n",
       "5          290    11.50         2.17             44.54       22.02        1.6   \n",
       "9          238     2.15        10.66             43.91        5.00        9.2   \n",
       "10         250     1.53        10.20             48.89        6.12        4.1   \n",
       "...        ...      ...          ...               ...         ...        ...   \n",
       "1218       241    10.16         1.66             40.60       19.10        2.9   \n",
       "1219        19     0.27         1.39              3.78        0.71        3.1   \n",
       "1220        96     6.84         5.23              4.32        1.89        1.2   \n",
       "1221       208    15.70         1.22             17.17       11.23        2.3   \n",
       "1222        67     1.22         2.28             14.30        4.43        2.0   \n",
       "\n",
       "      Saturated Fats (g)  Calcium (mg)  Iron, Fe (mg)  Potassium, K (mg)  ...  \\\n",
       "1                  1.898           279           6.04              126.0  ...   \n",
       "2                  2.275           299           6.59              138.0  ...   \n",
       "5                  2.313            14           0.91               76.0  ...   \n",
       "9                  0.630           684           4.89              127.0  ...   \n",
       "10                 0.000            20           2.76              165.0  ...   \n",
       "...                  ...           ...            ...                ...  ...   \n",
       "1218               1.507             4           0.78              572.0  ...   \n",
       "1219               0.053            62           0.90              327.0  ...   \n",
       "1220               1.486            70           1.15              269.0  ...   \n",
       "1221               2.452            30           0.49              309.0  ...   \n",
       "1222               0.244             3           0.27              132.0  ...   \n",
       "\n",
       "      Carotene, alpha (mcg)  Lycopene (mcg)  Lutein + Zeaxanthin (mcg)  \\\n",
       "1                       0.0             0.0                       63.0   \n",
       "2                       0.0             0.0                       66.0   \n",
       "5                       0.0             1.0                       42.0   \n",
       "9                       0.0             0.0                       25.0   \n",
       "10                      0.0             0.0                       88.0   \n",
       "...                     ...             ...                        ...   \n",
       "1218                  418.0             0.0                       29.0   \n",
       "1219                    0.0             0.0                     4204.0   \n",
       "1220                   12.0           313.0                     4097.0   \n",
       "1221                 2157.0             1.0                      162.0   \n",
       "1222                    6.0             0.0                      693.0   \n",
       "\n",
       "      Fatty acids, total monounsaturated (mg)  \\\n",
       "1                                      4530.0   \n",
       "2                                      5292.0   \n",
       "5                                      5797.0   \n",
       "9                                       393.0   \n",
       "10                                      290.0   \n",
       "...                                       ...   \n",
       "1218                                   4099.0   \n",
       "1219                                      7.0   \n",
       "1220                                   2402.0   \n",
       "1221                                   3498.0   \n",
       "1222                                    373.0   \n",
       "\n",
       "      Fatty acids, total polyunsaturated (mg)  20:5 n-3 (EPA) (mg)  \\\n",
       "1                                      1445.0                 12.0   \n",
       "2                                      1502.0                 13.0   \n",
       "5                                      2117.0                  0.0   \n",
       "9                                       973.0                  3.0   \n",
       "10                                      936.0                  0.0   \n",
       "...                                       ...                  ...   \n",
       "1218                                   4079.0                  0.0   \n",
       "1219                                    126.0                  0.0   \n",
       "1220                                   2112.0                  0.0   \n",
       "1221                                   9319.0                  0.0   \n",
       "1222                                    518.0                  0.0   \n",
       "\n",
       "      22:5 n-3 (DPA) (mg)  22:6 n-3 (DHA) (mg)  Caffeine (mg)  \\\n",
       "1                     0.0                  7.0            0.0   \n",
       "2                     0.0                  8.0            0.0   \n",
       "5                     0.0                  0.0            0.0   \n",
       "9                     0.0                  0.0            0.0   \n",
       "10                    0.0                  0.0            0.0   \n",
       "...                   ...                  ...            ...   \n",
       "1218                  0.0                  0.0            0.0   \n",
       "1219                  0.0                  0.0            0.0   \n",
       "1220                  0.0                  0.0            0.0   \n",
       "1221                  0.0                  1.0            0.0   \n",
       "1222                  0.0                  0.0            0.0   \n",
       "\n",
       "      Theobromine (mg)  \n",
       "1                  0.0  \n",
       "2                  0.0  \n",
       "5                  0.0  \n",
       "9                  0.0  \n",
       "10                 0.0  \n",
       "...                ...  \n",
       "1218               0.0  \n",
       "1219               0.0  \n",
       "1220               0.0  \n",
       "1221               0.0  \n",
       "1222               0.0  \n",
       "\n",
       "[1042 rows x 40 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       0.264550\n",
       "2       0.264550\n",
       "5       0.560000\n",
       "9       0.118750\n",
       "10      0.355556\n",
       "          ...   \n",
       "1218    1.058824\n",
       "1219    0.250000\n",
       "1220    0.750000\n",
       "1221    0.041000\n",
       "1222    0.171429\n",
       "Name: Price per Weight (£/100Gram), Length: 1042, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# command/\n",
    "#only normalising X not y. Is this right?\n",
    "# norm_X = preprocessing.normalize(X, axis=0) #collum instead of row\n",
    "# norm_X = pd.DataFrame(norm_X, columns = X.columns)\n",
    "# X=norm_X\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.ravel(y)\n",
    "X = X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 879us/step - loss: 47.4034 - mse: 47.4034\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 0s 854us/step - loss: 9.7790 - mse: 9.7790\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 17.0433 - mse: 17.0433\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 0s 787us/step - loss: 104.1217 - mse: 104.1217\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 0s 807us/step - loss: 16.2942 - mse: 16.2942\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 0s 790us/step - loss: 21.6383 - mse: 21.6383\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 0s 731us/step - loss: 22.7509 - mse: 22.7509\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 0s 830us/step - loss: 167.1295 - mse: 167.1295\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 0s 997us/step - loss: 7.8212 - mse: 7.8212\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 0s 984us/step - loss: 62.5848 - mse: 62.5848\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 0s 936us/step - loss: 71.1629 - mse: 71.1629\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 0s 844us/step - loss: 177.2336 - mse: 177.2336\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 0s 815us/step - loss: 138.9730 - mse: 138.9730\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 0s 877us/step - loss: 106.9938 - mse: 106.9938\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 0s 715us/step - loss: 6.6296 - mse: 6.6296\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 0s 764us/step - loss: 8.8197 - mse: 8.8197\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 0s 729us/step - loss: 106.5474 - mse: 106.5474\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 0s 743us/step - loss: 68.5931 - mse: 68.5931\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 0s 845us/step - loss: 88.3958 - mse: 88.3958\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 0s 981us/step - loss: 162.9326 - mse: 162.9326\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 0s 889us/step - loss: 16.0052 - mse: 16.0052\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 0s 934us/step - loss: 19.9510 - mse: 19.9510\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 0s 866us/step - loss: 14.2869 - mse: 14.2869\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 0s 905us/step - loss: 27.4267 - mse: 27.4267\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 0s 802us/step - loss: 5.7313 - mse: 5.7313\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 0s 800us/step - loss: 164.4783 - mse: 164.4783\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 0s 807us/step - loss: 145.2393 - mse: 145.2393\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 0s 776us/step - loss: 67.9728 - mse: 67.9728\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 0s 728us/step - loss: 18.6822 - mse: 18.6822\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 0s 730us/step - loss: 72.5203 - mse: 72.5203\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 0s 794us/step - loss: 56.1496 - mse: 56.1496\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 0s 814us/step - loss: 29.4871 - mse: 29.4871\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 0s 768us/step - loss: 237.9139 - mse: 237.9139\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 0s 818us/step - loss: 31.4563 - mse: 31.4563\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 0s 777us/step - loss: 172.5398 - mse: 172.5398\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 0s 780us/step - loss: 20.3307 - mse: 20.3307\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 0s 754us/step - loss: 14.8555 - mse: 14.8555\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 0s 856us/step - loss: 13.1138 - mse: 13.1138\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 0s 806us/step - loss: 26.1189 - mse: 26.1189\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 0s 930us/step - loss: 48.8103 - mse: 48.8103\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 0s 867us/step - loss: 98.9725 - mse: 98.9725\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 0s 854us/step - loss: 189.7294 - mse: 189.7294\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 0s 865us/step - loss: 186.7643 - mse: 186.7643\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 0s 822us/step - loss: 4.2005 - mse: 4.2005\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 0s 848us/step - loss: 87.3168 - mse: 87.3168\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 0s 919us/step - loss: 128.7544 - mse: 128.7544\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 0s 908us/step - loss: 16.1139 - mse: 16.1139\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 0s 841us/step - loss: 85.9150 - mse: 85.9150\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 0s 910us/step - loss: 111.2444 - mse: 111.2444\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 0s 899us/step - loss: 31.6221 - mse: 31.6221\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 0s 919us/step - loss: 27.9806 - mse: 27.9806\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 0s 877us/step - loss: 32.1649 - mse: 32.1649\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 0s 910us/step - loss: 80.4892 - mse: 80.4892\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 0s 845us/step - loss: 4.6732 - mse: 4.6732\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 0s 952us/step - loss: 36.2165 - mse: 36.2165\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 0s 900us/step - loss: 231.8083 - mse: 231.8083\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 0s 879us/step - loss: 232.4769 - mse: 232.4769\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 0s 987us/step - loss: 21.8757 - mse: 21.8757\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 0s 902us/step - loss: 74.6424 - mse: 74.6424\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 0s 907us/step - loss: 19.5055 - mse: 19.5055\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 0s 868us/step - loss: 13.9016 - mse: 13.9016\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 0s 928us/step - loss: 201.8308 - mse: 201.8308\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 0s 986us/step - loss: 62.0128 - mse: 62.0128\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 0s 858us/step - loss: 66.4333 - mse: 66.4333\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 0s 847us/step - loss: 22.2246 - mse: 22.2246\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 0s 979us/step - loss: 41.7309 - mse: 41.7309\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 0s 889us/step - loss: 18.0150 - mse: 18.0150\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 0s 918us/step - loss: 29.3937 - mse: 29.3937\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 0s 876us/step - loss: 185.8069 - mse: 185.8069\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 0s 949us/step - loss: 281.0989 - mse: 281.0989\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 0s 878us/step - loss: 165.7504 - mse: 165.7504\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 0s 857us/step - loss: 28.9566 - mse: 28.9566\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 0s 896us/step - loss: 7.7774 - mse: 7.7774\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 0s 931us/step - loss: 64.5259 - mse: 64.5259\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 144.8965 - mse: 144.8965\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 8.6001 - mse: 8.6001\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 11.9926 - mse: 11.9926\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 10.3287 - mse: 10.3287\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 13.1818 - mse: 13.1818\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 6.0063 - mse: 6.0063\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 0s 925us/step - loss: 15.9442 - mse: 15.9442\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 0s 919us/step - loss: 7.9524 - mse: 7.9524\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 980us/step - loss: 6.9195 - mse: 6.9195\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 0s 878us/step - loss: 160.3186 - mse: 160.3186\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 0s 879us/step - loss: 19.9910 - mse: 19.9910\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 0s 907us/step - loss: 85.1940 - mse: 85.1940\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 0s 896us/step - loss: 153.9886 - mse: 153.9886\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 0s 824us/step - loss: 104.8660 - mse: 104.8660\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 0s 977us/step - loss: 88.9943 - mse: 88.9943\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 18.9124 - mse: 18.9124\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 13.0349 - mse: 13.0349\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 18.9342 - mse: 18.9342\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 0s 993us/step - loss: 19.5836 - mse: 19.5836\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 9.6805 - mse: 9.6805\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 0s 967us/step - loss: 24.8985 - mse: 24.8985\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 0s 971us/step - loss: 9.6167 - mse: 9.6167\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 32.9759 - mse: 32.9759\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 10.6545 - mse: 10.6545\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 69.9565 - mse: 69.9565\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 14.2926 - mse: 14.2926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fac005d4b90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Feature Scaling/preprocessing - normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Building model\n",
    "model = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "model.add(Dense(8, activation = 'relu', input_dim = 40))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "model.add(Dense(units = 16, activation = 'relu'))\n",
    "\n",
    "#avoids overfitting\n",
    "#https://keras.io/api/layers/regularization_layers/dropout/\n",
    "model.add(keras.layers.Dropout(0.1))\n",
    "\n",
    "\n",
    "# Adding the output layer\n",
    "\n",
    "model.add(Dense(units = 1))\n",
    "\n",
    "\n",
    "# Compiling the ANN\n",
    "history=model.compile(optimizer = 'adam', loss = 'mse', metrics=['mse'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "model.fit(X_train, y_train, batch_size = 10, epochs = 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters involved:\n",
    "    https://keras.io/api/models/model_training_apis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 1ms/step - loss: 5.6118 - mse: 5.6118\n"
     ]
    }
   ],
   "source": [
    "#model evaluation\n",
    "mse = model.evaluate(X_test, y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE without tuning: 5.611782550811768\n"
     ]
    }
   ],
   "source": [
    "print('MSE without tuning: {}'.format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 8)                 328       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 489\n",
      "Trainable params: 489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/api/models/sequential/\n",
    "The Sequential Class, provides a training and inference features on this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEICAYAAABCnX+uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO2deXgV1fnHPyc7hD0shn1TENlkUVBEVHCru6XWqrWtda1Va9WqrRVbtdZqtVq1P9x3XHFHAQFRZF9EdgIJJCSEJBCyr/f9/XHu5N4kNyGQuybv53nuM3fmzp1z5syZ73znPWfOGBFBURRFiVyiQp0BRVEUpXmokCuKokQ4KuSKoigRjgq5oihKhKNCriiKEuGokCuKokQ4KuRKq8MY098YI8aYGPf8HGPM1Uewnb7GmCJjTLT/c6koTUeFXAlbjDFpxphSt1hmG2NeNsa083c6InKOiLzaxPxM9frfbhFpJyLV/s6TohwOKuRKuHO+iLQDxgDjgb94/2gsWo+VVo2eAEpEICJ7gDnAcGPMImPMQ8aYJUAJMNAY09EY86IxJssYs8cY86AT8jDGRBtjHjPG5BpjdgI/8d62e3u/9Zq/1hiz2RhTaIzZZIwZY4x5HegLfOq+Q7jLR4impzHmE2PMfmNMijHmWq9tzjDGvGuMec293Y3GmHEBLzilVaBCrkQExpg+wLnAWveiq4DrgPbALuBVoAoYDBwPnAk44nwtcJ57+Tjgp42kMx2YAfwS6ABcAOSJyFXAbtx3CCLyqI+/vw1kAD3daTxsjDnD6/cLgFlAJ+AT4L9NLgBFaQQVciXc+cgYkw98B3wDPOxe/oqIbBSRKqALcA5wm4gUi8g+4Ang5+51fwY8KSLpIrIf+Ecj6f0WeFREVoolRUR2HSqT7gvNJOBPIlImIuuAF7AXHIfvROQLd0z9dWBUE8tAURolJtQZUJRDcJGIzPdeYIwBSPda1A+IBbLcv4E1Kc46Peus35gw9wF2HEE+ewL7RaSwTjre4ZO9Xt9LgARjTIz7YqQoR4wKuRKpeA/bmQ6UA10bEMUsrEA79G1ku+nAoCakWZdMoIsxpr2XmPcF9jTyH0XxCxpaUSIeEckC5gKPG2M6GGOijDGDjDGnuld5F7jFGNPbGNMZuLuRzb0A3GGMGevuETPYGNPP/Vs2MLCBPKQD3wP/MMYkGGNGAtcAb/phFxWlUVTIlZbCL4E4YBNwAHgfSHb/9jzwFfADsAb4sKGNiMh7wEPAW0Ah8BE2Bg82tv4XY0y+MeYOH3+/HOiPdeezgftFZF6z9kpRmoDRF0soiqJENurIFUVRIhwVckVRlAhHhVxRFCXCUSFXFEWJcELSj7xr167Sv3//UCStKIoSsaxevTpXRLrVXR4SIe/fvz+rVq0KRdKKoigRizHG51PJGlpRFEWJcFTIFUVRIhwVckVRlAhHB81SlFZAZWUlGRkZlJWVhTorShNISEigd+/exMbGNml9FXJFaQVkZGTQvn17+vfvj9dQv0oYIiLk5eWRkZHBgAEDmvQfDa0oSiugrKyMpKQkFfEIwBhDUlLSYd09qZArSitBRTxyONxjpUKuBIXCQnhTR+ZWlICgQq4EhY8/hiuvhN27Q50TJVRER0czevRohg8fzvnnn09+fv4Rb6t///7k5uY2us4rr7zCzTff3Og6ixYt4vvvvz/ifIQLKuRKUCgvt9OKitDmQwkdbdq0Yd26dWzYsIEuXbrwzDPPhDpLKuSKcji4XLWnSutm4sSJ7NnjeZ3pv/71L8aPH8/IkSO5//77a5ZfdNFFjB07luOOO46ZM2cecrsvv/wyxxxzDKeeeipLliypWf7pp59y4okncvzxxzN16lSys7NJS0vjf//7H0888QSjR4/m22+/9bleJKDdD5WgUF1de6qEkNtug3Xr/LvN0aPhySebtGp1dTVff/0111xzDQBz585l+/btrFixAhHhggsuYPHixUyePJmXXnqJLl26UFpayvjx47n00ktJSkryud2srCzuv/9+Vq9eTceOHTnttNM4/vjjAZg0aRLLli3DGMMLL7zAo48+yuOPP84NN9xAu3btuOMO++a+AwcO+Fwv3FEhV4KCOnKltLSU0aNHk5aWxtixY5k2bRpghXzu3Lk1oltUVMT27duZPHkyTz31FLNnzwYgPT2d7du3Nyjky5cvZ8qUKXTrZgcHvOyyy9i2bRtg+9FfdtllZGVlUVFR0WD/7KauF26okCtBwXHiKuRhQBOds79xYuQHDx7kvPPO45lnnuGWW25BRLjnnnu4/vrra62/aNEi5s+fz9KlS2nbti1Tpkw5ZN/qhrrt/f73v+f222/nggsuYNGiRcyYMaNZ64UbGiNXgoIj4BpaUTp27MhTTz3FY489RmVlJWeddRYvvfQSRUVFAOzZs4d9+/Zx8OBBOnfuTNu2bdmyZQvLli1rdLsnnngiixYtIi8vj8rKSt57772a3w4ePEivXr0AePXVV2uWt2/fnsLCwkOuF+6okCtBQR254s3xxx/PqFGjmDVrFmeeeSa/+MUvmDhxIiNGjOCnP/0phYWFnH322VRVVTFy5Ejuu+8+JkyY0Og2k5OTmTFjBhMnTmTq1KmMGTOm5rcZM2Ywffp0TjnlFLp27Vqz/Pzzz2f27Nk1jZ0NrRfuGBEJeqLjxo0TfbFE6+Kxx+DOO2HFChg/PtS5aX1s3ryZY489NtTZUA4DX8fMGLNaRMbVXbfJjtwY08cYs9AYs9kYs9EYc6t7eRdjzDxjzHb3tHOz90BpcagjV5TAcTihlSrgjyJyLDAB+J0xZhhwN/C1iBwNfO2eV5RaaK8VRQkcTRZyEckSkTXu74XAZqAXcCHgtAq8Clzk70wqkY/2I1eUwHFEjZ3GmP7A8cByoIeIZIEVe6B7A/+5zhizyhizKicn58hyq0Qs6sgVJXActpAbY9oBHwC3iUhBU/8nIjNFZJyIjHM67CutB3XkihI4DkvIjTGxWBF/U0Q+dC/ONsYku39PBvb5N4tKS0AduaIEjsPptWKAF4HNIvJvr58+Aa52f78a+Nh/2VNaCurIFe9hbKdPn05JSckRb2vRokWcd955AHzyySc88sgjDa6bn5/Ps88+e9hpzJgxg8cee+yQ67Vr167R3480/cPhcBz5ycBVwOnGmHXuz7nAI8A0Y8x2YJp7XlFqoY5c8R7GNi4ujv/973+1fhcRXEdQQS644ALuvrvhznLBENLGCCshF5HvRMSIyEgRGe3+fCEieSJyhogc7Z7uD2SGlchE+5Er3pxyyimkpKSQlpbGsccey0033cSYMWNIT09n7ty5TJw4kTFjxjB9+vSaR/e//PJLhg4dyqRJk/jwww9rtuX9Aons7GwuvvhiRo0axahRo/j++++5++672bFjB6NHj+bOO+8EGh4296GHHmLIkCFMnTqVrVu3+sx7amoqEydOZPz48dx33301y4uKijjjjDMYM2YMI0aM4OOPbXCibvoNrdccdNAsJSjoWCvhQ4hHsaWqqoo5c+Zw9tlnA7B161Zefvllnn32WXJzc3nwwQeZP38+iYmJ/POf/+Tf//43d911F9deey0LFixg8ODBXHbZZT63fcstt3Dqqacye/ZsqqurKSoq4pFHHmHDhg2sc+90Q8PmJiYmMmvWLNauXUtVVRVjxoxh7Nix9dK49dZbufHGG/nlL39Z6+UYCQkJzJ49mw4dOpCbm8uECRO44IIL6qVfVVXlc73mvFNVhVwJCurIFWcYW7CO/JprriEzM5N+/frVjKOybNkyNm3axMknnwxARUUFEydOZMuWLQwYMICjjz4agCuvvNLniyYWLFjAa6+9BtiYfMeOHTlw4ECtdRoaNrewsJCLL76Ytm3bAjZk44slS5bwwQcfAHDVVVfxpz/9CbChoXvvvZfFixcTFRXFnj17fL6YoqH1jjrqqMMozdqokCtBQR15+BCiUWxrYuR1SUxMrPkuIkybNo2333671jrr1q1rlmP1pqFhc5988skmp+FrvTfffJOcnBxWr15NbGws/fv39znsblPXOxx09EMlKKgjV5rChAkTWLJkCSkpKQCUlJSwbds2hg4dSmpqKjt27ACoJ/QOZ5xxBs899xxg30RUUFBQb6jahobNnTx5MrNnz6a0tJTCwkI+/fRTn2mcfPLJzJo1C7Ci7HDw4EG6d+9ObGwsCxcuZNeuXYDvoXJ9rdccVMiVoKC9VpSm0K1bN1555RUuv/xyRo4cyYQJE9iyZQsJCQnMnDmTn/zkJ0yaNIl+/fr5/P9//vMfFi5cyIgRIxg7diwbN24kKSmJk08+meHDh3PnnXc2OGzumDFjuOyyyxg9ejSXXnopp5xySoNpPPPMM4wfP56DBw/WLL/iiitYtWoV48aN480332To0KEA9dJvaL3moMPYKkHhuuvg+efhrbfg8stDnZvWhw5jG3kEZBhbRWkO6sgVJXCokCtBQZ/sVJTAoUKuBAV15KEnFGFU5cg43GOlQq4EBXXkoSUhIYG8vDwV8whARMjLyyMhIaHJ/9F+5EpQUEceWnr37k1GRgb6LoDIICEhgd69ezd5fRVyJShoP/LQEhsby4ABA0KdDSVAaGhFCQr6ZKeiBA4VciUoqCNXlMChQq4EBXXkihI4VMiVoKCOXFEChwq5EhS014qiBA4VciUoaD9yRQkcKuRKUFBHriiBQ4VcCQrqyBUlcKiQK0FBHbmiBA4VciUoqCNXlMChQq4EhWA78gUL4IEHgpOWooQaFXIlKAS7H/kHH8ATTwQnLUUJNSrkSlAI9pOd1dUaxlFaDyrkSlAItiOvqlIhV1oPKuRKUAiFI6+qCk5aihJqmizkxpiXjDH7jDEbvJbNMMbsMcasc3/ODUw2lUhHHbmiBI7DceSvAGf7WP6EiIx2f77wT7aUlkawe61UVdm09M1mSmugyUIuIouB/QHMi9KCCXY/cu23rrQm/BEjv9kYs94deunc0ErGmOuMMauMMav0vYGtj1A4clAhV1oHzRXy54BBwGggC3i8oRVFZKaIjBORcd26dWtmskqkESpHrg2eSmugWUIuItkiUi0iLuB54AT/ZEtpaagjV5TA0SwhN8Yke81eDGxoaF2ldRNsR65CrrQmYpq6ojHmbWAK0NUYkwHcD0wxxowGBEgDrg9AHpUWQLAduYZWlNZEk4VcRC73sfhFP+ZFacGEoh+5d7qK0pLRJzuVoBCKJztBHbnSOlAhV4KCOnJFCRwq5EpQCJUjVyFXWgMq5EpQCJUj19CK0hpQIVeCgvYjV5TAoUKuBAV9slNRAocKuRIU1JErSuBQIVeCgo5+qCiBQ4VcCQqhcuQaWlFaAyrkSlDQsVYUJXCokCsBR8Tzpp5gj7WiQq60BlTIlYDjLd4aWlEU/6NCrgQcb/HWxk5F8T8q5ErA8RZTdeSK4n9UyJWAEwpHro2dSmtChVwJOKFw5BpaUVoTKuRKwNHGTkUJLCrkSsDxdsXBcMgiwR82V1FCiQq5EnCC7ci9xVsdudIaUCFXAk6wHbm3eKsjV1oDKuRKwAmlI1chV1oDKuRKwAl2rxVvR66hFaU1oEKuBJxg9yNXR660NlTIlYCjjlxRAosKuRJw1JErSmBRIVcCjiOmMTHBd+Qq5EprQIVcCTiOeMfGBr/7oYZWlNZAk4XcGPOSMWafMWaD17Iuxph5xpjt7mnnwGRTiWQc8Y6N1e6HzWHVKsjICHUulHDkcBz5K8DZdZbdDXwtIkcDX7vnFaUW3o5cQytHzvTp8PDDoc6FEo40WchFZDGwv87iC4FX3d9fBS7yU76UFoS3Iw92Y2dLCq0UFdmPotSluTHyHiKSBeCedm9oRWPMdcaYVcaYVTk5Oc1MVokk1JH7h4oK+1GUugStsVNEZorIOBEZ161bt2Alq4QBwXbkLbWxs7LSfhSlLs0V8mxjTDKAe7qv+VlSWhrBduQttbGzslIdueKb5gr5J8DV7u9XAx83c3tKCyTYvVZaYmhFxO6XOnLFF4fT/fBtYCkwxBiTYYy5BngEmGaM2Q5Mc88rSi2C3Y+8JTZ2OgKuQq74IqapK4rI5Q38dIaf8qK0UNSRNx9HwDW0ovhCn+xUAo4+2dl8HAFXR674QoVcCTjBHmulJTZ2qiNXGkOFXAk4dXutiAQ2vZYcWlFHrvhChVwJON4xcgi8kLfExk4NrSiNoUKuBBxvR+49HyhasiPX0IriCxVyJeDUdeSBFteW2NipoRWlMVTIlYATbEfuXCiio1uOI3ecuDpyxRcq5ErACZUjj4trOUKujlxpDBVyJeCEypHHx7e80Io6csUXKuRKwKnryIPV2Bkf33IcufZaURpDhVwJOHUdebBCKy1JyL1DK4HuvqlEHirkSsDxfrITIiS0kpEBr73mtzw1F0fIRVrOxUnxHyrkSsCJSEf+2mtw9dVQWuq3fDUH79i4hleUuqiQKwEn2DFyvzjykhI7DRMh9xZvbfBU6qJCrgSciHTkZWV2GoZCHkpHvn8/nH8+7NN3gR2SJ56A884LTloq5ErACaUjb7aQO9MQEy6hlR9+gM8+g9WrQ5eHSGHNGli6NDhpqZArASdUY600K7QSxo48lKGVMLu+hTXl5VBcHJy0VMiVgBORT3aGmWKFS2glzIolrCkrs2IejIfSVMiVgBOKJzujo213x5biyL1duDryyKC83E6D4cpVyJWAEwpHHhPTzEGzwkyx1JFHHk4ZqZArLYJQOXK/CHmYOPJwi5GHSbGENY4jLyoKfFoq5ErACcVYKzExzQytOEoVJtYzXHqtqCNvOhpaUVoUoehH7rfQSphYTw2tRB4aWlFaFKEYa8VvoZUwUaxwC62ESbGENRpaUVoUoXLkLbXXSjg48jAplrBGHbnSogjFk53qyAODv4pl9mx49tnm5yecUUeutChcLjDGCiuoI28KmzZBly6we7edb2kx8pdegv/8p/n5CWeC6chj/LERY0waUAhUA1UiMs4f21VaBtXVEBXlEfJg9VqJZEe+dSscOAA7d0LfvuEXWmlusRQXB+/x9VARqb1WThOR0SriSl1cLiuqUVGe+UDS7NCKSEiEvLoannzSJuncCDjTykpPY3EoQyuOODW3WEpKWraQV1V56l4wQit+ceSK0hiOI3eEPOxDK1VVnqtNEEMrK1fCH/4AgwZ5hkN3ppWVkJgIBw+GhyNvbrG0dEfuXPAgshy5AHONMauNMdf5WsEYc50xZpUxZlVOTo6fklUiAceRByu00mxH7m03g+jIHedWXFzfkVdUQNu2nu+hwl83KiUl9oLUUl+S4V0+kSTkJ4vIGOAc4HfGmMl1VxCRmSIyTkTGdevWzU/JKpFAxDly77MwiI7cOeFLSjzJ1nXkzvdQ4c8Yufe0peHtyCOm14qIZLqn+4DZwAn+2K7SMgi2I/du7Dyi9ELkyL2FvO6b5ryFvCU48pYu5BHnyI0xicaY9s534ExgQ3O3q7Qc6jryYIZWnPnDIgwduXdoJRwceXOKRcSzXy1VyCPRkfcAvjPG/ACsAD4XkS/9sN3WTXW1rfEtgLqOvDmhlYqKQ4dLvEMrzvxhEa6OfPvamu+hwh+OvLzcczGvEXIRSElpVt7CiYhz5CKyU0RGuT/HichD/shYq6a8HJKT4d13Q50Tv+BPR37BBXDzzYdOzy+O3JiwceSVlRCXn0OMqYr40IqzT+AlckuWwNFHw48/HvmGwwjHkcfGRoiQKwEgJ8d+tm4NdU78Qt1+5M1x5Js2Hdq4+c2Rd+wYNo68okKIrS4l1lT7duQFBZCWFvA8+kPIvYWt5ntGhp1u2XLkGw4jnPJJSoqc0Iribw4csNPCwtDmw0/488nO3FyrWY1Rt7HziB15585BdeTefcfrOfIKIU7KiTWVvh35Qw/B5HqdxfyOt5AfaeTPpyN3Dmp6+hHnLZxwHHlSkjry1ksLE3J/OXJH4A5VLH4LrXTqFHBHXlICZ54JGzcewpGXC7FUEmcqfTvyzEz7CXC7ilMcLteRx+q9ha3GrbYwIfd25CrkrYDych9Cs3+/nbYQIfeXI8/Ls9OmOvJmh1aC4Mh37IB58+C77w4RI6+wQh4rDTjygwdtQQfwwiNi62u7dnb+SJNqTY68Sxd7sQp0vwUV8hAzYQI8+GCdhY4jD0ZwLQj4a6wVR8iD5sg7dw64Iz940DP1JeQ1vVYqhDgqiKO84Ri59zQAOOLUqZOdHmnR+IyRtzAh93bk3kP3BAoV8hCzdauPNqoWFlrx15OdjpAXFTV+MfBbjNwJrQTQTvkS8uLi+mOtVFRgHbmrwqeQl+eXkk/HgAq59/XNe74p7NvnOQ6NOnJn3N5AMnduwNPxjpFD4D2ZCnkIqaiwjqveCdHChNxfT3bm5tqpSONxR8eR+yW0ArWf7vBBauphbt+L/Hw7PaQjrzTu0Eo5FRX1LywP7bqS8awMipA7jrypUafCQhg4EN580877dOROXc/ODvyjq5dcYoeZDCB1hTzQcXIV8hDiuLF6OtHChNzfjhwaLxq/OnJoVLHWr7citXz5YabhxtuRe7vweqMfVuEOrVRQWVp/hzaV9GcnA3HlB17IO3asPX8o9uyxQuZ0G200tCJi/xAoSkttot6VKQB4h1ZAhbxFc0ghb2Excn85cmjceB5OY+f+/fDAA3WOQV0hb0SxHDd+pK68qTHyiqoo68ippKK0zg6JsLeyCy6iyd8buGDskcbI9+2zU0c7nYtTp051eq047wIMZJzcObecW6EAoaGVVoRTlwIWWtmyBZ54onnb8AP+erKzqY78cBo758yBGTPggw+8FpaV2auA0z2jEUfuXFycjkaHiyPk+fm+ux+WlFiTWu2yQh5HBZVldQqwvJwsOcrmJzNwYYkjjZFnZ9upc/yc/ezWrY4jP+YY+z2QQu4cqAALeVmZfTDYueipI2/BNMmRN6eh7dVX4fbbAxo3bQr+6kd+uI68KULubNOJ3wL2LExIgDZtPPMN4IjTkd6p+3LkhYWeu4jSUk9/7TgqiKWSyvLaOyQHC9iLFfK87CN9SemhOYwblVrUFfKSEmu+O3euI+THHWe/B8OROwUfIMrLIT7e4wVUyJtLcTEsXBjqXAD2KeTzz/eYAWfaoJC7XLWb+OHwuwoAZGUddl79iT/7kcdFWVVriiNvSmjFEfKvvrKjIgAeIU9IsPNNcOT+FHLnwtOmjT38TttfTWilrPbFvTCzkBLsGLd5OYEbWvJIGzvrhlaKi+1IjomJdYQ8Odmq+yGEPDu7GcM8BCm04lQhZ+jhQDd3tXwhf/llOP10z1gOTSQQF+xvvoHPPoO1a2un4VPIjbHfvWtAaip06NBwy1pGRu2MO8qUmel7/ZUrbVeshn73E0125F9+CX//e4Pbyc2F/sZ2G2vwxHC5qCoqJWb/viY58rw8z5uE3nnHvTCIjtzRk337bDl16eL5zemD7MRXa0IrFbXFOmunR1Fz95sjy0gTqOfIU5tmEHw58sRELyF3uewB7dABevdu9FwtKYHBg2HmzCPciSDGyOPjbfgIPBezQNHyhdxpKt+2rcl/Wb/eGoOf/9y/jduOMXYOqqO5tXRCxFa2nj3tvHcryebN9j57g4/h3isrYfx4uPtuzzInIV9CvXUrnHACnHUWXHbZ4e3I1q3wxhtNXr0pjnz3bqh66TV45JF64aR162DNGsjLEwZU2+PZYGglPZ2qChcxO7c12ZEPHmzfk7lokXthWZkV8cNw5M2NkTtJeL88y2koc9ZxQisVdS78e9M9HcvzDkQfMs0jfVi1Xoz8habVgUM68uJie8w7dIAePbxujeqTkWFPiSPtJVQj5AUFAR0Yv6zMCnlSkg0jBfqmOOKE3OWyvQyaXDC7dtnp9u1NTmP1aluv3nsPrr22/u+ZmUcW83L01KnYPkMrzssM+/a1897W09npvXvrb3z+fLt882bPssYcudPNYuDAw+9y8fTTcPXVTe7v29CTna+/bh9RLyyEoUPh+dVj7P7Xsds33ww//Snk5QoD2AlA4cEGTsItW6gmmuiSwiY78q5dYcAALyNYWhr0GLlDY0IeG2PFvLKy9oUuK91zpco72Pj71L/5xjrqw7xBBXw48pymxQscR15cbOu6tyMvKsJzVe7QwRZAI/bVqcobNzaeZt3ozEUXwb/+heeKKxLwp2ATEuyNdXKyO9+pqTB2rB2Pwc9EnJBv2mR7GdTcBh8KR8gPY9D6lBQbX73ySlvmddsbTzgB/va3Jm+uhoYcebl3LwTHMfTpY6feouYIuC8hnzXLTp399U4oM5PPP4dbb/WRmRNPtGfa4biTzEy7fhOHTfXVj7y4GH75S3jmGdttuLQU1u7rVTtvbnbutOdAYVEUvdhDNFUU7K3TduCwZQtVxBBT2jQhz821gtmnj9fJfxgxcn8IufGKhvgSckdvYju2tY68snb4xKkOCZSSVxTXaHpr19rr76GE0Bc1Qh5vy6P0QNOeenWEHKyOOo68XTu3IXLqePv20L17k4R88+aGq+ySJdYHffutnRexbSCLFuE5vyAg4ZVt22zDuePIwd5cZ2W5M7Zmjb1g+ZmIE3LnZGuyiXTE5jAceUqKdWjjx1tT6/18QkGBnfc2vo2RlwfnnmsdkKNPjlGuceT5Xo7PqWiNOXLvMwOs0MyebRUhI8MqV2mpJyyTmckbb8BTT3k5MWdbxx9vYw/eXUIOhXM27dzZpNV99SPfscOzKScr24rd4SSvC1V5eW1d70ouHSigMNv3LZFs3kI1MUQXFzQptOI48j59bDqVlRxWjNwfjZ1OFA0OEVrpnGh7rVTVFvKsfVHEUc6AuD3kFrettW91cc4f7+t9U6kR8mJ7QpRVRTcpprRvH/Tq5clTcXGdGLm3I+/e3db5OmX+4Yf2VHaqXnFxw0/ZO2GX997zpFlW5v5vgIX86aetQSkq8viAGke+ZIndR6d3jh+JOCF3Dt7OjaXwww+eHyor4e23a1+mCwtrDtyPG0ytY1iLN96AP/2pZjYlxcZMx4yx82vWeFZ1ToSm9pD6/nvbV3n+/PqhlYN5VmHKqrzimnWF3DtG3pAjnzfP7uull1rVysysHWfMzCRlS732ekYAACAASURBVGXNqoBVrY4d7RXLmW8qzrqOGrv3syHB9OXInRukWkIuR9fLi3O8HdeaRB7tKaQwx/dj864tti0kpjj/kI5cBPJyXSQtfJ/enYsRcSftFnJXXALTmMuny7r5/L/LZXUsKspqwuH2pKiutofNOdRwiNBKu3jiol31hHxvbixHsZdubYvJK7NCnpJiw81f1nnpolNvj2SoEUdbOxywV4EyEg4ZoykpsVX42GPtfF5e7dBKVRVU5LnNihNagVr1t7gYpk+Hxx6rHSXctMl3mk4T0kcf2WPs7Gs9IQ9Aj4b0dFsvUlMbcOQTJngcjR+JWCFPXZkDEyfalkmwIv6LX3i1WFFjO4p7HcOJO97kgRkN3Aa+8Ya9xxepeXXg4MEwapQVEKeXiXf6TT0RnBuCbdt8hFb2WTEqd8UiLnfemuLI6wr58uW2clxxhWe/nROhXTtkTybbN7mFfE6lZ1vJyfbjve1DIcLeLGExp9QI+ebNcPLJdR6q8cKXI3dukLyFPIueFNKuVl4c53jBBXaaRB4dKKBgv+/BsKu22CtETNFBoqNsmWZk+H7xTEkJlJVHkZS6kj4ZSwG30LmFPH1/IvOZxsdr+vhMKz/f7ku/7iWIHL7Bc4xok4W8bSyxcfYpT2+y8hNINntJSiwnr7w9YD1OdbV1st44uuvTkVdXwznnWOfhA0fI22SnkUCpFfJDPE7v1PVhw+zUceROYydAcY47TOY4cqgl5Bs32nLeutXWF6eMGgoPbdhg61p6um0ody5e+/ZBRW6Bp2C9Dti2VQUMiMvgy3+s9bHFpuOklZZW25Hn50Ppjyn2RAkAkSvkRd3ILO3E0HGJfPdVMXzyif3BOxbuVtGFQ2+klLasXuq7ce6LH/vwn+JrYN8+8vLsCTZ4sK1oQ4f6duROrO9QOEK+erXHXNc0droduRBF1V73PXpjQu4IeN3Qytq19kwZMsTO79rlSWTkSPZnlXOwoi0xVDLvK5e9aWlAyFetgl69pGFd37+fhyvvYBrzKN9m1WDjBiuY2+5+EW66qd5fmurIAbZzdK0LlVN+M2bA3WPmclL7DbSnyHdjZ34+67OtEHR3ZRFTYpXyttuEM8+sv7oTeuhKLn02fwXUFvJtu+2ZuCmzk8+icP4/ZO83teabiiPQTRXyuLYxxMVCZXVtR5dVkEhybB5J7crIrbIDoezYbsvnyzmuWmHshhz5Bx/AmJGVVH4531pZHzhCHr9nJwmUNcmRO1XVlyOveVgm190G0aEDszcPZQkneervJ5+w/if3AB4hHzbMVltfQu5y2eWXXWbr20cf1b573psb47kLdQt5cTFceomLtMrefPJe4wOkHQqnOFyu2o4cIIuj4KSTmrX9hog4IXcOSomrDW/0vZetlYO48coCqr6cb3/wDp67bccccy4A6zdF12+bqa7mnqzfcxePUrIxtUZgBr//CGzbxpgxtYXc+wRoSnjFEaIlS+y0c2eP2fC+syv/0YYE9meUsIwT6zd2Ovf9UVH2SuP9oNCaNTYO5CiCtyMfNYqUMhugvIiPyC2I54wz4Kof7sB1VM/aQl5QwKKb3ycz07Dstlk1vVI++8wripKZySaGUUE827ZYsdj6ld3JXXsT4Lnn6nWPdBy5MfbjcnmEvLTUOvooY7e1LXa4R9mrqtj1/gqio4Xhw+EfA2aS0LsrHRLKKShyV93duz1dFrdu5TluJDGmjOm8R/RGe7dWUmJIT69vHp34dhJ59F76PuA+ER0h32UbDjdld/HZpuf8/xjssQuUkNc0dibGERsfRYWrtpDvLW7PUQn5JHWoIs/VGRHYudQqaHpGVM3dSFWVp2jrCvnXX8PaTQlsYHhNzGLWLE8ziDOmdmwsRGfsIiGqglLaNirkGzZ4/t+oI8+zV4h86cgVDxzNX3jQI+Tz57M+19bR9HRbb3r2tGFmX6GVtDR7apx2mu2UsGBB7fN0z4G25Pc6jiqia4T8wQdhY3oH+rCb5am+w2i1mDLF0823urqmwbe8vHY7rbcjB9hkhnPTrMlNbl87HCJOyHfvhs6d7En/RuVlRBkXG3KTebb4l3aFOkIucfHM2TrA9nQojqlXgXd8n816GUkF8Xz/VaFHyL97GS6/nDEjq8jI8OjikQq5495HjbL1p6IC8otiMNh9KdtolfK+2WOYzGKK47vYmuDY+MJCq3pDh9p5x+pkZbEu+yhOXPgI6fsTbcudtyMfPZoUBgNwU9/PaUsxmzYJbxRfzL/3XGYb9Dp2tGf4z3/O1uW2cm94dyPceivl5XbUz5qXX2RlsQ07JsaG3R1AhG3L7V3ErrGX2DP08cdrlYHjyMFOndBKW3e73OrVMLpHFgYXW7tN8qjNwoWkfbWVXh2LbMNlVhYcdRTt27ooLLEtmfLU0+Tf8whs3cqB1Tt5m8u5YuJOOlJAzLLa3bxWrKh9bGocefsKOh5Io33b6lqOfHuKzfTB8jY+Owo5/w+YkOdYBT6Yb4Uitl08sfFRVHoJeVkZ5JZ3IDmxgK6dqqgkjqKD1ezYXk0vrMh+OcfWsb177bHo2tXq77599nk5EY/gruAE2LyZrCy4/HLrDy65xLrLL75wi9OuXbSJq6asTecGhby01HYW+O1v7Xz//va/dWPkAEX7rWF47aMOlJZFsZHjPCfc5s2sZ2TNdrOyagt53Z4rjocYPhzGjyxn7VohLc3TxpJR0IFh85/iFp6C/HxE4O23hXNi5nMVr7M+v2+9h6lrkZFh+3B+/bUtuBEjak6OukahriN/vePNPPdiXMNtdc0gooS8utqW4+RR9gz4MasbU08q5RQW8z9zI5xySq2eFO98m8zf2j9Kanosl0e9C8D6HwR+//uas3r2W/a2Lpoqvl6SQMo2F1FU07+PC9as4aQF9knDuXPtNtPTPS3wTYmTp6V5BAuskIM9iQ6WJ5AUZY9q+ZZUROCzTQOoJI4166JsdyzHkbvFLXvoqQh4wg9r1/Ip57Ni91HceSfQr5/HkcfFwZAh7GAQBhcT/34u++hO1nMfczEfcu+Sc23FT062O7NgAVt6ng7Axn7nwjvvsH5NVa1nkIrTckjHKs+PlUOssO+0oro7pw385je2/5VXrXYcOVghLyqyPzt3mdnZMCAuk77Re9gWNcQj5GvXsot+9I/O8JRBcjLtO0BBhT1LXnk/kV7sIWfxZt54P4Ey2nDjzTY/0Us9Qm6MDyHfba+uST87A4yhd9v9VpdKSiA+nm3bIMp9ofXl/uo68v25h/eAiSPkPXt6yqeWkP/6fLtevt1uXLs44hKicBFd07D6/fd2enzXdJKSpGa/dmS2YTKLGcpmvnjLXpwd43HSSdad3367PVzbt3v8z3JOhLw89qfY3igxMbahPjbWNkc5Qp4QL5S16dRgjPzHH+1FxhHF7lG5JCXZaltZWceR51ci8Qn87wV73HLozr5Ue2xkkxXyCbGrarbd65PnGGY2U1xc30w59XTYMBj71cMUFxsWLIDjjrJX2W+qTyaruAMzuY6UtBjWrIFduwyXVs1iQvJuqohlzcpGWq0dIdi0CdmTybmbH+O/L1rrXfeaVuPIO1mN+ajwdDp1sncK/iaihDw721aCyf08LTUnn5nIWadXsVmO5cDAsbZGfvstqe1GcMWy3zMj7xaiouCu4V8AsP7rHPjvf+GFFwD4cF57xrCaCWY5C7b0ZMPCHPqQTvzjD8N993HC3IfoFZXJ+2/Y27/du20bqzENOPKqqppgYkGBjaVPmeL52RHytDSolmh6tLH3zeXbd7NxXSW7y3oA9un5WkK+dy//5Xcc9eGz/IaXKM9wO5Y1a1iD7V7zzjswJ/4ijyPv3h169SKFwfSOzyHhknNJjK0k6tWXmcl1JMZXc9ddWCFftAjKy9lSaO3DRjkODhxg9SzbKuk0OKVs8HQL28Bw5PulbC3pXVM28ofbbeF4PWHq7cijoz0Nnd4vfU92ZXBMuyy2lfeHvXt5+mn4zf9OIJUB9Cv80bqfvXshOZkOnaIprGoDeXl8umskJSTy9WelfLa2J8e2SWX01K42rRTraHuaTMaMkXpCnrvJ3rUknTocxo6lT3Uq6TvKrcIOGcK2bTCpr71ab/qhfuNqXpaNpx7Ty6pV3jar7FOmwB/+4FlPql3kbthbr8+109bWsaNnjO+uXT2/d8GK6cH9Vlhi28UTm2AV3xlIa948iKGSKX12kJRkbWfWzlJ2F3RiUJcD/DT+Mxas6cSePR6hmTTJTt+13oYffvDcOa6IsVfX/B/sOfbWW5D/5TIua2PboBIS7HjhCW0M2VHJFO323f1w7Urb/vMH/s3Z5ksSzptKUqeqmnMmMdEzHMHmzA7MSzifzZvtXQDAxpR47v5DGX/NvJ4DdOHSylk12+65fRHHbbdx/I0bsc7AfTu0fr31Mh32bmPcLhsuy82F0VlfEEc5n2BbzV1E8dfFZ/D++xBtqrkw5gtOvNGeR8u/aqTV2hHykhKWPrGMOZzLA7t+RWleSU35Hu3ufOU48qSt3xNLBRXVMUyb5hkDyJ9ElJA7leAYk0Iyth/SxIkw8c/WRa6InmiP2iuv8EjpLURTzYrfzmTdOhhxejcGmp388J1bGJcvZ+9eWLqjOxebjzi993ZW5g3gg+96cH78PNtN4m9/I+qbhVzqeo8582MoKLB5GDTIap9PIb/jDvv0lkhNzwCnoS0hwXOQHTHr3t4KY3lKOl+8ZF1258RyKzpeQv7W+3H8nv9yXP8iXuHX/OLh4VYYVqxgTcwJXHCBzde53/+FS7Y9QmlWvrV3PXuSYo5hcM9S27p00knwxRd0JY8/X7WbOXNgAadDQQF5dCG3MIH27WFrVnsq23Rg1Vx7opaW2mvk1u22yhw7qJwNDCfv3sc5QBcG9i6nrAz2JQ6wZfDGG/Dtt7hc9prS3namICrKNlpBHSEvS+OYrvvZVtADycvjnVkuXk6dQgZ96F+2BZYutZlITqZ9UhyFtKf6o09ZxBQAPl3RncX5Iznr2N22ISI2lhismJwiiznx2AJWrrRmYOsWQTZsJC/F3g11GTcQJk+mz8GNpKdZ0awYfzJpaTBpTDFdyGPTd/vh7LM9A7ClpZG3JYcYKulz3TlEUU3epn2kpto7748/9uzbg+ctpceIbvyx4wuUfLm4ZrnjyB0hT0iwJ39srBBHOe2wYbUCJ7TSPoG4NvWFfGL8WtonxZHU3f62dnU11RLNoEHwq4vzcUkUr71YWVNfTx5VVGsbc+faUF8vMthUdQwFtCd/s70r6pT+I1Fnn8mlef9n63B0Fbhc9O5RyeLc4+i9ZZ7PZ+3Wvr+Dzuzn8Vm9mfNlFGzcSJcDO2vy0LYtjB5t3enfV57DbSUPM3AgPPSQ/f2rlIH888kEHuQ+ACawnL4drcD2JJNhKfbCsnEj9rZi4EDy5q3h00/dxmn2bIayhbZYZ9+PXfQkk3T6Eh9bze1Jr/H2zgn8859wWuJKkiYdS/fTjmMAO/lyjvDN/23hm/mVfDOvgjWPLUCqqq0jmTevplPB628Yoqgml2688Uh6jZBPnGinjiOPWrSAo7Dn9lln1S8rfxBRQr57l63QfYs3MyAmHWPsg4njx1sTuKxgGAIseGsvL8uv+M0N8Yx//jpGjABOPJGR8gNrNsSSQ1fYsIE5s62Inn/UKqaO3IcQxVTzNY//drPncjppEj9NWkR5VQwvv+x5er5PH09opaTEdi+sKHPZFqJNm2DbthqXM+H9O2gXV07P9oV0f+dpALZvtiLTo6sVjrKMXL74uIJRrOOMKS7ryNu1g4MHyXzyXW58/nhOYTGrF5fwAPfz4bqBLJ/6Z3I//Z7dVb2YNMn2OLn/nBXMdl3IXd+cy5a2Y/jP823Z3H48g09zN55Om1bT4fvmG6vp2xd+u+ZGtnE0WztPAOC88+yrxbaf8htWpXSkWzdb7hs2wNbd9iGZi6bHkcpAVqdaW3XmT2zD4K5dwL332kK64gqWv7uL3FybLFhHnppqj9eoUdCxrY2PJudv5pjeJRSUJ7CP7qRs84Qp+rHLM85scjId+nZCiGLpHR9wgC4kxpQxK/s0ymjDWWdH2Y336EF7CokxVUxjHid02VHTZ3vosYbeIzrx7ZfFdOIAMUcPgFNOobdrF9kHEzghaiV/fG0U1dUwZGx7hrGJTd/l2ccDr7vODg88YAC5s78liTyirricLuwnb2d+Teep1FQbdcjZJzz61Wh6x2Tz78JrueMaj4P1FvJOnTwhuLbx1bSlhLiJ4zC4OJhjyyiuQ0LNBTE11XqWNWtgmpkPHTvStac9Bt8ttYI+aFgCg357OpP5hpeeK2f3bmgbX8XIs5Jr8tCli23MBriMdxCiWJ0wifwUe3fR6c+/g+7dmToql/ZRRSSIDRO8849UPv/1+xTRjplPu3t67NxpnzpbuJC1yysYnZiC+dl062R+8xuSsjbURD57dinDvPcu/4q9lz2lXdhcOZj//MfG0rvEFvBs6jkAnM0cEtu6GHlye46psvGtnmTSeesyko9y2TulTz6BggL+e/5XlJRYH8Hs2USPG8PoPra8+0wdSk+3+Rs1qJhHhr7Ci0Mf5fgRlfy+6GFbQYcM4RS+Zf66rky5YShTpsUy5cw4xt55On++cIM9wfbvh9tuo5w4Zu07jZ/Hf8TxrOGJV5PYvdseR6dhNz51C1x/PXz+OT3b2TvvQAk5IhL0z9ixY+VIeGzi+wIiByadJ/ckvyRnnun5bcQIkWkn5stZzBEQ6ZZYJGlpXn9OTZUHuVesjRV5jutl+qnZ0jN2n7imnCauh/8hn3OOFJIosnZtrXSrrrxakk2WdO3qEhD55LkMmX5hufTvL/L768ulQ3ypgEhUlEviKJMR/CBr7nxLnnrKppVtesg4VshkFskBOgqIXNrtGwGRW6dtFBBZGjdZYimXP3R8QR591P7vrgHvyqkskpP5VhIokZS4Y0VcLilI6i9J5Mg5fC5zf/2mgMj8+e7M5ufLbR1eFBCJNlWe/X3O/fuKFVKzMD9fli0T6ZpYLF3IlbsG2fJ9/3378yt/WCfRVMqtfezyB6/aIld2myN94vfKRx/Zda6/ukRA5IMP7Py777rTWbdOpHt3uTv2XxJDhez/3V9EXC7p2NGud+4x20Tuu0+Oxe7/HM6SL2/+VEDkM84VELmal2X0gAOypdfpIrGx9o9ffy3PPWuPwx08KiDy5zOXC4jEUyrF2/fY9MeNEwHZ8eBbUo2RtD8+Je3aifzspHR5nmuks9kvIDIobpddPydHPuNciaFC+sTvrSmipYtK5VpmSjeyReLjRUA2M0SOjk6RGCrkuKhNIi6XDGm7S37W7jM57dQqaWcKBUTeeTRN7vjFHomiSjb97T25bvwaiaNMMpfZNO/8Y7XEx7tERGTKFJE+fUTE5ZLkxIOSzB6RlSslkUJpF2PLOO2VhZJz+8PSliK56iqXvPGGzeOyqIki994r5Yu+lz7sEmNs+ex5+gORykp5pf3NAiJduogMSdwtApJEjpzQZ4+cf76nOqxgnIDIv3o/Kf8d/ERN3ZXly0Ueekju5mG5KflDkW7dRMrLRd56Sy5ktvToUi77nnxTPuUn4gKpJFoSKJE/nLXBcxKlpclDUX+W7tE58lTUrVKNsYn26CHX85xc2+/LmlUnd99sj02nHHHFxUvRwSqRZ5+V3/G0gEhxv2NFQKYenyvjB+WKgOx68kPpYvLk/KQlItu3220/9JD8/qZKAZHPP62W6W0+ERC56Wf7RM47T+T440Xeesuuu3y5iMsl+Z36ygKmyIK4s+yUKfJb84KAyOikXXK8WSMbluTLC53/KCDy1aS/yes977Ka001k+HCRN9+0m5zR9pGawv31qFVywglHJHu1AFaJD031izADZwNbgRTg7kOtf6RCfuuFO6UdBeICkZ/9rNZv113nqZAPc7cUz/++9p9dLinp1lc+4Tw5qVeqdCNbOiaUym/bvC5yzTVWgcAe3Lq89prMZarExVYLiKyLGy+3d3zeijdVchWvyov8Wu7r9qzcxT+lV9QeiYuqkCF9S6QNxeK6515Z93mG/PBhirhyciU2qlJGsdbWtWt2WCH7nRWxfxz/jixc6NmXHmTZ5VH3iPTrZ/MzYoQ8gq08p59u18vL82S3dPEKudh8KLeMWyK7dolkZoq4XO4fq6pEOncWadOmZmHK4x9JF3IFROLiRIqKRKKiRCZOtNueHXWxDGCH/Jy3ZDwrZOpR62XHDvtbQoLV2JwcO/+vf9nvLpeIbN0qwzplyOld1tgf//c/6dzZfTHkPBGQM7qstmX6/nbZuaVcQGpO2Pe5RCQtTeS990R+/WuRe+4RKS2VTz91px1bJUf3K5Ntb1ohnxq/2FMIP/mJXWn9epGBA0UuvFBcb71tM3vCCfLkI/bie2LP3Z7/DBsm1Rgp/NPf5VirFZKbK3JPp2cllnKRa6+V0gt+JiNjNkhS5yq5bexi+fTiF0VE5OxRmZJIoUSbKrmDR6UtRXJB9KcSb8rkl9Gvixw8KDu+zZAoquTKUT/IA9fvkaTo/TKQFJEePeSSwevkuO77RAYOlEFsl0FtMkREpGt0noDIWFaKfPutyCOPyB94XKKjXdKhg8jA5GKpIkrkySdFfvyx5iLYhmJxrVwlIiKV190kN8b8n4DIWXwpcscd8sqJz8oiJss9J39TU48rfnaFJCaK3H7sFzWmp+zu+23ZeBuA22+3y3Jy5OOOVwmIdMbm85E/7pMNt84UEHntpcra59Ett4h06CBy000i//ynyIcfilRXi+zdK1JQULPajaOWCIj8YcBs69BERLKzZYMZLk9yi91XY+SWCcskMbpU3upwvSQmuqRtXIWsYoxNo317kZ07a3R661aRW0d8LSDy4qM5IldeKTJggK1XnTvb80JEZMIE+4fnnxe5+26R66+XqvtmyO08Lqeab6RbwkEZNEgkMbpEJrFYqv72kJT/+nrpRYaAyNlj9sq339pN/IM/ibzwgshll0npxh1SWFhfWg6XgAk5EA3sAAYCccAPwLDG/nOkQj73K5c82e/fNtt/+lOt3156yS6+KOYTcbVpax1DXS64QATk22d+qKmTH3CxyEMPifz4o13w9NP1/5eZKQLyyfTXZFqPdVKa0Em+SrxYxrJSvul3lcjSpR7VO+UUyf3VH+WXMW+KoVpGxmyQukewZ0+R2CjrFF54PF9A5PXXrMA9fXeGFBSIHHWUyLXHLLJu8s3vxPXZ5yJvvGE3MH26lIw8USacaC8s/fv7KKyNG0Xy830X5JVXiowa5Zlfvlxe40oBkeOOs4vGjrW7Ex0tsufbHXL+5APSt+0+aUeB3DR6ibhcIv/5j8jkySJXX22Fu317kaFD7f/69hW5+GL7/cl/V4ucfbZIdLR0jdkvfdglVW+8LZKaKlddafc7O9ueS3FxLukfs9uKe/tJXlcgD07anTvbc811IF+u4lWZffKjnpWuuUYkJsbWg0su8YjQSSeJHDggFRUiw4aJXH6514avv96uM2eOpKWJvP66XfzAMW8IiFTNmSsP3G/vcj77rHaedu+slKkJi8VQLauOuVxOP9leKNpSJOk/vc1T9D3m1mTlzLiFsvammSLTp8vG6BGyhIkip50mI3rlyfBhVlj69iiz9YTf2DvFefMkg54SH1UuQ/sWye6Ow+2OHDwokpVl3V/sa3ImX3rE0a0s33S5SDYxVGTDBlvYv/qVvMEvBET6Re0Syc6W/v1FrjonR+4Y87W0ia/ylL9jAMDWLTeVc+ZJL9Klu8mWM08tFWNEJk2yq/34o48DV13tu0568exFXwmILEz8ichll3l+mDrVbjglReS44+T/ku8XEImJqpKTThJJTakSOfFEkV69RH74QURscitX2r8/eke2rVerq0RuvlmkXTuR5GSRSy/1pHHrrSKDB9fWj5SUmvrzzaupEh0t0q1NgWTQ01aE776TR/o/JyBybdQLsuuWxwREnjju+UPu6+ESSCGfCHzlNX8PcE9j/zlSIRcRG0MAq9xe5OXZC3322HOsG/PFc8+J9OghUloqp3bfJLGUy8FugzwV85tvPFfmukye7BGDBx6wTu8f/xApLra/r1plVe+JJ2riDJvbjpGt766rt6kbbrC3YNf/ulyWLbObfOIJO33lFbtOZaWIfP21yIwZ9fNSUSFSWio5OfYc/s1vmlBu3hQWiuzbV2uRa8tW+dWvPMkVF4ts2iSyY4edv/dem7+khEL5/oWN4ovhw+06Y8aIXHihyJAh9pORIfaicvvt8ru4/5PXT3ux5j8zZtjzyTm/hw3zFHPRk42fCLV04aGHRL73ugtbs8a6IRGRnTvtsX/lFXu74aagQKSkxGuDCxfaC5yXOxQRefScBTY/Byrk8stFBg1qID//ekz2kCzy5pvy17+6q8ptebUu5PlfLJH5Zzwse/8+09p9h717bT7FmkLnNnzIEJEOCWVSFN/Fc8wefVS2M0gKaCfStasNJTg89ZS4omPE1btP7cy98IKtn94XcJdL1j1r3e9pI3NExEakzj5b5Le/tRpXi+uuEznrrHr7nf6/zyR3zgopLhY5/3x7A3TWWe46fAQUvjBLXuNKcR0zpEaQRcQenxtusAf+hhvkO04SEOmeVCl797rXKSkRKS31ud20NJH773fXmb/8xR6g+HiROXM8K1VWes5pb6ZOtZVa7Gm58cm5Im3bWgciIvv3iyR1qZanj3pQXCD/7feo7Pk+7cgKoBECKeQ/BV7wmr8K+K+P9a4DVgGr+vbt27y9WbpUpKzM929ZWbXjDN64XFYERST16x3y5S2f1zqxG6WkxN5uXX99nbPfi9RUWxGKikR+9SuRJUsOuVnnRsCpV++/37TsOFRW+jStfic7W+Tll+vdXNTi0ktFkpJE0tMb2VB1da0MFxaKbN7s+fmii2w51BORoqmWBQAACI9JREFUEPLUf1w1YZaLLvLc7dejvLwmXLBzp40k+NKEQ3HVVSK/+IX9/te/2ruPevX9rbesYTh4sP4G1q+3oZC6rF4tsm1brUWlpVbfr7nGzp9zjhXz6dNFjj328PPuF8rLRT7+uOFzXEQkP1+Kvv9BTj3VJfPmHUEa27ZZVd+9+5Criog90byvTC5XvZOhqEikeu8+kQULAnZSBlLIp/sQ8qcb+0+zHHkLY9s2exRuuMFOv/oq1Dk6cnJyDiHiTeCuu2w5TJ7snzz5g5kzbZ4yMqxbHT8+sOlVVzcpAuE3Xn7Zar+IvYj06ycybZqNFirhRUNC7o+u6RmA9/BwvYHAvgSyBeH0cnSeEnS6l0Ui3g+zHCnH2Kf/GTy4+dvyF05/4LIy25XdGaY8UEQFuVPwr37l+d6tm30ouHt3z1gvSvjjjyqzEjjaGDPAGBMH/Bz4xA/bbRU4IuEIuTMiXGvFGcDReXAqHHCEu7Q0OEIeSrp1s89FZGZ6XummhD/NduQiUmWMuRn4CtuD5SUROYIXSbVOHEfuDLgUyY7cH4webYfMCdiDE0eAtyN3Xh7UUnHuqvbsUSGPJPzy1L+IfAF84Y9ttTbqhlZauyNv1w4WLz70esEk2KGVUOI9aJcKeeQQUY/ot0Ti3O/KbQkx8pZKawqteLdzqJBHDirkISYqyop5ZaUdFc0RdiV8aE2hFXXkkYkKeRjghFfat/cMgK+ED60ptKKOPDJRIQ8DvIVcCT8cIW8NoZVOnTwvuVAhjxxUyMMAR8hbe0NnuOIId2GhfblGSxbyqChP/3HnZRdK+KNCHgY4jk8deXjiHB/nXYstOUYOnji5OvLIQYU8DFBHHt7UFfKW7MhBhTwSUSEPAzRGHt60NiF3GjxVyCMHFfIwQIU8vHG6iLam0Ep8fMvfz5ZEAN7nrBwuzgmjoZXwJSGh9Tjym26yL0VWIgcV8jBAHXn405qEfPhw+1EiBw2thAHa2Bn+tGnTekIrSuShQh4GqCMPf1qTI1ciDxXyMECFPPxJSICiIvtdhVwJN1TIwwBt7Ax/vMVbhVwJN1TIwwB15OGPd1xcY+RKuKFCHgZoY2f44y3e6siVcEOFPAxQRx7+qJAr4YwKeRigMfLwx1u8NbSihBsq5GGAOvLwxxHvuDj7yL6ihBNaJcOA8eNh0iTPONBK+OEIuYZVlHBEhTwMOP10+PZbiI0NdU6UhnAEXIVcCUdUyBWlCTiOXOPjSjiiQq4oTUBDK0o4o0KuKE1AQytKOKNCrihNQEMrSjijQq4oTUBDK0o40ywhN8bMMMbsMcasc3/O9VfGFCWcUCFXwhl/vCHoCRF5zA/bUZSwxRFwDa0o4YiGVhSlCagjV8IZfwj5zcaY9caYl4wxnRtayRhznTFmlTFmVU5Ojh+SVZTgoUKuhDOHFHJjzHxjzAYfnwuB54BBwGggC3i8oe2IyEwRGSci47p16+a3HVCUYKDdD5Vw5pAxchGZ2pQNGWOeBz5rdo4UJQzR7odKONPcXivJXrMXAxualx1FCU80tKKEM83ttfKoMWY0IEAacH2zc6QoYYgKuRLONEvIReQqf2VEUcIZ7X6ohDPa/VBRmsBRR8H998NFF4U6J4pSH388EKQoLR5jYMaMUOdCUXyjjlxRFCXCUSFXFEWJcFTIFUVRIhwVckVRlAhHhVxRFCXCUSFXFEWJcFTIFUVRIhwVckVRlAjHiEjwEzUmB9h1hH/vCuT6MTstES2jQ6Nl1DS0nA5NMMuon4jUGwc8JELeHIwxq0RkXKjzEc5oGR0aLaOmoeV0aMKhjDS0oiiKEuGokCuKokQ4kSjkM0OdgQhAy+jQaBk1DS2nQxPyMoq4GLmiKIpSm0h05IqiKIoXKuSKoigRTkQJuTHmbGPMVmNMijHm7lDnJ1wwxqQZY340xqwzxqxyL+tijJlnjNnunnYOdT6DiTHmJWPMPmPMBq9lPsvEWJ5y16v1xpgxoct58GigjGYYY/a469I6Y8y5Xr/d4y6jrcaYs0KT6+BijOljjFlojNlsjNlojLnVvTys6lLECLkxJhp4BjgHGAZcbowZFtpchRWnichor/6sdwNfi8jRwNfu+dbEK8DZdZY1VCbnAEe7P9cBzwUpj6HmFeqXEcAT7ro0WkS+AHCfaz8HjnP/51n3OdnSqQL+KCLHAhOA37nLIqzqUsQIOXACkCIiO0WkApgFXBjiPIUzFwKvur+/CrSqt02KyGJgf53FDZXJhcBrYlkGdDLGJAcnp6GjgTJqiAuBWSJSLiKpQAr2nGzRiEiWiKxxfy8ENgO9CLO6FElC3gtI95rPcC9TQIC5xpjVxpjr3Mt6iEgW2MoIdA9Z7sKHhspE61ZtbnaHBV7yCsm1+jIyxvQHjgeWE2Z1KZKE3PhYpn0nLSeLyBjsbd3vjDGTQ52hCEPrlofngEHAaCALeNy9vFWXkTGmHfABcJuIFDS2qo9lAS+nSBLyDKCP13xvIDNEeQkrRCTTPd0HzMbe8mY7t3Tu6b7Q5TBsaKhMtG65EZFsEakWERfwPJ7wSastI2NMLFbE3xSRD92Lw6ouRZKQrwSONsYMMMbEYRtePglxnkKOMSbRGNPe+Q6cCWzAls3V7tWuBj4OTQ7DiobK5BPgl+4eBxOAg85tc2ujTjz3YmxdAltGPzfGxBtjBmAb81YEO3/BxhhjgBeBzSLyb6+fwqsuiUjEfIBzgW3ADuDPoc5POHyAgcAP7s9Gp1yAJGxr+nb3tEuo8xrkcnkbGxqoxLqkaxoqE+zt8DPuevUjMC7U+Q9hGb3uLoP1WFFK9lr/z+4y2gqcE+r8B6mMJmFDI+uBde7PueFWl/QRfUVRlAgnkkIriqIoig9UyBVFUSIcFXJFUZQIR4VcURQlwlEhVxRFiXBUyBVFUSIcFXJFUZQI5/8BdqNMpJSRv3QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "plt.plot(y_test, color = 'red', label = 'Real data')\n",
    "plt.plot(y_pred, color = 'blue', label = 'Predicted data')\n",
    "plt.title('Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cite: https://stackoverflow.com/questions/49008074/how-to-create-a-neural-network-for-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 1.0124326516671986\n",
      "Mean Squared Error: 5.611782387048243\n",
      "Root Mean Squared Error: 2.3689200887848125\n"
     ]
    }
   ],
   "source": [
    "#Why are my errors all of a sudden so because of overfitting?\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kerastuner\n",
    "from kerastuner import HyperModel\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('units', min_value=8, max_value=32, step=4), \n",
    "        activation = hp.Choice('dense_activation', \n",
    "                values=['relu', 'tanh','sigmoid']),#,'softmax']),\n",
    "        input_dim = 40\n",
    "        )\n",
    "             \n",
    "    )\n",
    "\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('units', min_value=8, max_value=32, step=4), \n",
    "        activation = hp.Choice('dense_activation', \n",
    "                values=['relu', 'tanh','sigmoid'])#,'softmax'])\n",
    "        )\n",
    "             \n",
    "    )\n",
    "    \n",
    "    model.add(\n",
    "        keras.layers.Dropout(\n",
    "            hp.Float(\n",
    "                    'dropout',\n",
    "                    min_value=0.0,\n",
    "                    max_value=0.1,\n",
    "                    step=0.01)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Adding the output layer\n",
    "    model.add(Dense(units = 1))\n",
    "\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        #optimizer = hp.Choice('dense_optimizer',\n",
    "                #values=['adam','SGD','rmsprop','adadelta'] ),\n",
    "        loss = 'mse',\n",
    "        metrics = ['mse']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_rs = kerastuner.tuners.RandomSearch(\n",
    "            build_model,\n",
    "            objective='mse',\n",
    "            max_trials=5,\n",
    "            executions_per_trial=2, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 02s]\n",
      "mse: 63.07194709777832\n",
      "\n",
      "Best mse So Far: 62.40811347961426\n",
      "Total elapsed time: 00h 00m 10s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner_rs.search(X_train, y_train, epochs=10) #, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "7/7 [==============================] - 0s 979us/step - loss: 1.3827 - mse: 1.3827\n"
     ]
    }
   ],
   "source": [
    "#best_model = tuner_rs.get_best_models(num_models=1)\n",
    "best_model = tuner_rs.get_best_models(num_models=1)[0]\n",
    "mse_rs = best_model.evaluate(X_test, y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 804us/step - loss: 1.6133 - mse: 1.6133\n"
     ]
    }
   ],
   "source": [
    "loss, mse = best_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in ./untitled_project\n",
      "Showing 10 best trials\n",
      "Objective(name='mse', direction='min')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 20\n",
      "dense_activation: relu\n",
      "dropout: 0.09\n",
      "Score: 62.40811347961426\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 20\n",
      "dense_activation: tanh\n",
      "dropout: 0.08\n",
      "Score: 62.55263328552246\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 32\n",
      "dense_activation: sigmoid\n",
      "dropout: 0.08\n",
      "Score: 62.96870231628418\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 20\n",
      "dense_activation: sigmoid\n",
      "dropout: 0.09\n",
      "Score: 63.0499153137207\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 12\n",
      "dense_activation: relu\n",
      "dropout: 0.09\n",
      "Score: 63.07194709777832\n"
     ]
    }
   ],
   "source": [
    "tuner_rs.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
