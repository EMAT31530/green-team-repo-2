{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import tensorflow\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>Food Group</th>\n",
       "      <th>Price (£)</th>\n",
       "      <th>Weight (GRAMS)</th>\n",
       "      <th>Price per Weight (£/100Gram)</th>\n",
       "      <th>Carbon Group</th>\n",
       "      <th>Land use (m2/100g)</th>\n",
       "      <th>GHG(kgco2eq/100g)</th>\n",
       "      <th>Water use (L/100g)</th>\n",
       "      <th>Acidifying emissions(kgSO2eq per 100g)</th>\n",
       "      <th>...</th>\n",
       "      <th>Carotene, alpha (mcg)</th>\n",
       "      <th>Lycopene (mcg)</th>\n",
       "      <th>Lutein + Zeaxanthin (mcg)</th>\n",
       "      <th>Fatty acids, total monounsaturated (mg)</th>\n",
       "      <th>Fatty acids, total polyunsaturated (mg)</th>\n",
       "      <th>20:5 n-3 (EPA) (mg)</th>\n",
       "      <th>22:5 n-3 (DPA) (mg)</th>\n",
       "      <th>22:6 n-3 (DHA) (mg)</th>\n",
       "      <th>Caffeine (mg)</th>\n",
       "      <th>Theobromine (mg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Waffles Buttermilk Frozen Ready-To-Heat</td>\n",
       "      <td>Baked Foods</td>\n",
       "      <td>1.50</td>\n",
       "      <td>567.0</td>\n",
       "      <td>0.264550</td>\n",
       "      <td>Bread products</td>\n",
       "      <td>0.3482</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>4530.0</td>\n",
       "      <td>1445.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Waffle Buttermilk Frozen Ready-To-Heat Toasted</td>\n",
       "      <td>Baked Foods</td>\n",
       "      <td>1.50</td>\n",
       "      <td>567.0</td>\n",
       "      <td>0.264550</td>\n",
       "      <td>Bread products</td>\n",
       "      <td>0.3482</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>5292.0</td>\n",
       "      <td>1502.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Dutch Apple Pie</td>\n",
       "      <td>Baked Foods</td>\n",
       "      <td>2.80</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>Bread products</td>\n",
       "      <td>0.3482</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>5797.0</td>\n",
       "      <td>2117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Bread White Wheat</td>\n",
       "      <td>Baked Foods</td>\n",
       "      <td>0.95</td>\n",
       "      <td>800.0</td>\n",
       "      <td>0.118750</td>\n",
       "      <td>Bread products</td>\n",
       "      <td>0.3482</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>393.0</td>\n",
       "      <td>973.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Bagels Wheat</td>\n",
       "      <td>Baked Foods</td>\n",
       "      <td>1.60</td>\n",
       "      <td>450.0</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>Bread products</td>\n",
       "      <td>0.3482</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1218</td>\n",
       "      <td>Plantain Fried</td>\n",
       "      <td>Vegetables</td>\n",
       "      <td>0.90</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1.058824</td>\n",
       "      <td>Other vegetables</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>...</td>\n",
       "      <td>418.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4099.0</td>\n",
       "      <td>4079.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1219</td>\n",
       "      <td>Romaine Lettuce Raw</td>\n",
       "      <td>Vegetables</td>\n",
       "      <td>1.00</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>Other vegetables</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4204.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>Palak Paneer</td>\n",
       "      <td>Vegetables</td>\n",
       "      <td>3.75</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>Cheese</td>\n",
       "      <td>8.0642</td>\n",
       "      <td>2.1240</td>\n",
       "      <td>473.5</td>\n",
       "      <td>0.014894</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>4097.0</td>\n",
       "      <td>2402.0</td>\n",
       "      <td>2112.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1221</td>\n",
       "      <td>Carrots Raw Salad</td>\n",
       "      <td>Vegetables</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.041000</td>\n",
       "      <td>Other vegetables</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>...</td>\n",
       "      <td>2157.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>3498.0</td>\n",
       "      <td>9319.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1222</td>\n",
       "      <td>Corn on the cob</td>\n",
       "      <td>Vegetables</td>\n",
       "      <td>1.50</td>\n",
       "      <td>875.0</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>Other vegetables</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>693.0</td>\n",
       "      <td>373.0</td>\n",
       "      <td>518.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1042 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name   Food Group  Price (£)  \\\n",
       "1            Waffles Buttermilk Frozen Ready-To-Heat  Baked Foods       1.50   \n",
       "2     Waffle Buttermilk Frozen Ready-To-Heat Toasted  Baked Foods       1.50   \n",
       "5                                    Dutch Apple Pie  Baked Foods       2.80   \n",
       "9                                  Bread White Wheat  Baked Foods       0.95   \n",
       "10                                      Bagels Wheat  Baked Foods       1.60   \n",
       "...                                              ...          ...        ...   \n",
       "1218                                 Plantain Fried    Vegetables       0.90   \n",
       "1219                             Romaine Lettuce Raw   Vegetables       1.00   \n",
       "1220                                    Palak Paneer   Vegetables       3.75   \n",
       "1221                               Carrots Raw Salad   Vegetables       0.41   \n",
       "1222                                Corn on the cob    Vegetables       1.50   \n",
       "\n",
       "      Weight (GRAMS)  Price per Weight (£/100Gram)       Carbon Group  \\\n",
       "1              567.0                      0.264550    Bread products    \n",
       "2              567.0                      0.264550    Bread products    \n",
       "5              500.0                      0.560000    Bread products    \n",
       "9              800.0                      0.118750    Bread products    \n",
       "10             450.0                      0.355556    Bread products    \n",
       "...              ...                           ...                ...   \n",
       "1218            85.0                      1.058824  Other vegetables    \n",
       "1219           400.0                      0.250000  Other vegetables    \n",
       "1220           500.0                      0.750000            Cheese    \n",
       "1221          1000.0                      0.041000  Other vegetables    \n",
       "1222           875.0                      0.171429  Other vegetables    \n",
       "\n",
       "      Land use (m2/100g)  GHG(kgco2eq/100g)  Water use (L/100g)  \\\n",
       "1                 0.3482             0.1441                56.7   \n",
       "2                 0.3482             0.1441                56.7   \n",
       "5                 0.3482             0.1441                56.7   \n",
       "9                 0.3482             0.1441                56.7   \n",
       "10                0.3482             0.1441                56.7   \n",
       "...                  ...                ...                 ...   \n",
       "1218              0.0310             0.0455                 8.3   \n",
       "1219              0.0310             0.0455                 8.3   \n",
       "1220              8.0642             2.1240               473.5   \n",
       "1221              0.0310             0.0455                 8.3   \n",
       "1222              0.0310             0.0455                 8.3   \n",
       "\n",
       "      Acidifying emissions(kgSO2eq per 100g)  ...  Carotene, alpha (mcg)  \\\n",
       "1                                   0.001209  ...                    0.0   \n",
       "2                                   0.001209  ...                    0.0   \n",
       "5                                   0.001209  ...                    0.0   \n",
       "9                                   0.001209  ...                    0.0   \n",
       "10                                  0.001209  ...                    0.0   \n",
       "...                                      ...  ...                    ...   \n",
       "1218                                0.000531  ...                  418.0   \n",
       "1219                                0.000531  ...                    0.0   \n",
       "1220                                0.014894  ...                   12.0   \n",
       "1221                                0.000531  ...                 2157.0   \n",
       "1222                                0.000531  ...                    6.0   \n",
       "\n",
       "      Lycopene (mcg)  Lutein + Zeaxanthin (mcg)  \\\n",
       "1                0.0                       63.0   \n",
       "2                0.0                       66.0   \n",
       "5                1.0                       42.0   \n",
       "9                0.0                       25.0   \n",
       "10               0.0                       88.0   \n",
       "...              ...                        ...   \n",
       "1218             0.0                       29.0   \n",
       "1219             0.0                     4204.0   \n",
       "1220           313.0                     4097.0   \n",
       "1221             1.0                      162.0   \n",
       "1222             0.0                      693.0   \n",
       "\n",
       "      Fatty acids, total monounsaturated (mg)  \\\n",
       "1                                      4530.0   \n",
       "2                                      5292.0   \n",
       "5                                      5797.0   \n",
       "9                                       393.0   \n",
       "10                                      290.0   \n",
       "...                                       ...   \n",
       "1218                                   4099.0   \n",
       "1219                                      7.0   \n",
       "1220                                   2402.0   \n",
       "1221                                   3498.0   \n",
       "1222                                    373.0   \n",
       "\n",
       "      Fatty acids, total polyunsaturated (mg)  20:5 n-3 (EPA) (mg)  \\\n",
       "1                                      1445.0                 12.0   \n",
       "2                                      1502.0                 13.0   \n",
       "5                                      2117.0                  0.0   \n",
       "9                                       973.0                  3.0   \n",
       "10                                      936.0                  0.0   \n",
       "...                                       ...                  ...   \n",
       "1218                                   4079.0                  0.0   \n",
       "1219                                    126.0                  0.0   \n",
       "1220                                   2112.0                  0.0   \n",
       "1221                                   9319.0                  0.0   \n",
       "1222                                    518.0                  0.0   \n",
       "\n",
       "      22:5 n-3 (DPA) (mg)  22:6 n-3 (DHA) (mg)  Caffeine (mg)  \\\n",
       "1                     0.0                  7.0            0.0   \n",
       "2                     0.0                  8.0            0.0   \n",
       "5                     0.0                  0.0            0.0   \n",
       "9                     0.0                  0.0            0.0   \n",
       "10                    0.0                  0.0            0.0   \n",
       "...                   ...                  ...            ...   \n",
       "1218                  0.0                  0.0            0.0   \n",
       "1219                  0.0                  0.0            0.0   \n",
       "1220                  0.0                  0.0            0.0   \n",
       "1221                  0.0                  1.0            0.0   \n",
       "1222                  0.0                  0.0            0.0   \n",
       "\n",
       "      Theobromine (mg)  \n",
       "1                  0.0  \n",
       "2                  0.0  \n",
       "5                  0.0  \n",
       "9                  0.0  \n",
       "10                 0.0  \n",
       "...                ...  \n",
       "1218               0.0  \n",
       "1219               0.0  \n",
       "1220               0.0  \n",
       "1221               0.0  \n",
       "1222               0.0  \n",
       "\n",
       "[1042 rows x 51 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nutrition = pd.read_pickle(\"./Nutrition_Full_Features.pkl\")\n",
    "nutrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data for features\n",
    "X = nutrition.iloc[:, 11:]\n",
    "y = nutrition.iloc[:, 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Calories</th>\n",
       "      <th>Fat (g)</th>\n",
       "      <th>Protein (g)</th>\n",
       "      <th>Carbohydrate (g)</th>\n",
       "      <th>Sugars (g)</th>\n",
       "      <th>Fiber (g)</th>\n",
       "      <th>Saturated Fats (g)</th>\n",
       "      <th>Calcium (mg)</th>\n",
       "      <th>Iron, Fe (mg)</th>\n",
       "      <th>Potassium, K (mg)</th>\n",
       "      <th>...</th>\n",
       "      <th>Carotene, alpha (mcg)</th>\n",
       "      <th>Lycopene (mcg)</th>\n",
       "      <th>Lutein + Zeaxanthin (mcg)</th>\n",
       "      <th>Fatty acids, total monounsaturated (mg)</th>\n",
       "      <th>Fatty acids, total polyunsaturated (mg)</th>\n",
       "      <th>20:5 n-3 (EPA) (mg)</th>\n",
       "      <th>22:5 n-3 (DPA) (mg)</th>\n",
       "      <th>22:6 n-3 (DHA) (mg)</th>\n",
       "      <th>Caffeine (mg)</th>\n",
       "      <th>Theobromine (mg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>9.22</td>\n",
       "      <td>6.58</td>\n",
       "      <td>41.05</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.898</td>\n",
       "      <td>279</td>\n",
       "      <td>6.04</td>\n",
       "      <td>126.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>4530.0</td>\n",
       "      <td>1445.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>309</td>\n",
       "      <td>9.49</td>\n",
       "      <td>7.42</td>\n",
       "      <td>48.39</td>\n",
       "      <td>4.41</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.275</td>\n",
       "      <td>299</td>\n",
       "      <td>6.59</td>\n",
       "      <td>138.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>5292.0</td>\n",
       "      <td>1502.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>290</td>\n",
       "      <td>11.50</td>\n",
       "      <td>2.17</td>\n",
       "      <td>44.54</td>\n",
       "      <td>22.02</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.313</td>\n",
       "      <td>14</td>\n",
       "      <td>0.91</td>\n",
       "      <td>76.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>5797.0</td>\n",
       "      <td>2117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>238</td>\n",
       "      <td>2.15</td>\n",
       "      <td>10.66</td>\n",
       "      <td>43.91</td>\n",
       "      <td>5.00</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.630</td>\n",
       "      <td>684</td>\n",
       "      <td>4.89</td>\n",
       "      <td>127.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>393.0</td>\n",
       "      <td>973.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>250</td>\n",
       "      <td>1.53</td>\n",
       "      <td>10.20</td>\n",
       "      <td>48.89</td>\n",
       "      <td>6.12</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>20</td>\n",
       "      <td>2.76</td>\n",
       "      <td>165.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1218</td>\n",
       "      <td>241</td>\n",
       "      <td>10.16</td>\n",
       "      <td>1.66</td>\n",
       "      <td>40.60</td>\n",
       "      <td>19.10</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.507</td>\n",
       "      <td>4</td>\n",
       "      <td>0.78</td>\n",
       "      <td>572.0</td>\n",
       "      <td>...</td>\n",
       "      <td>418.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4099.0</td>\n",
       "      <td>4079.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1219</td>\n",
       "      <td>19</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.39</td>\n",
       "      <td>3.78</td>\n",
       "      <td>0.71</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.053</td>\n",
       "      <td>62</td>\n",
       "      <td>0.90</td>\n",
       "      <td>327.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4204.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>96</td>\n",
       "      <td>6.84</td>\n",
       "      <td>5.23</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.89</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.486</td>\n",
       "      <td>70</td>\n",
       "      <td>1.15</td>\n",
       "      <td>269.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>4097.0</td>\n",
       "      <td>2402.0</td>\n",
       "      <td>2112.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1221</td>\n",
       "      <td>208</td>\n",
       "      <td>15.70</td>\n",
       "      <td>1.22</td>\n",
       "      <td>17.17</td>\n",
       "      <td>11.23</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.452</td>\n",
       "      <td>30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>309.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2157.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>3498.0</td>\n",
       "      <td>9319.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1222</td>\n",
       "      <td>67</td>\n",
       "      <td>1.22</td>\n",
       "      <td>2.28</td>\n",
       "      <td>14.30</td>\n",
       "      <td>4.43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.244</td>\n",
       "      <td>3</td>\n",
       "      <td>0.27</td>\n",
       "      <td>132.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>693.0</td>\n",
       "      <td>373.0</td>\n",
       "      <td>518.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1042 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Calories  Fat (g)  Protein (g)  Carbohydrate (g)  Sugars (g)  Fiber (g)  \\\n",
       "1          273     9.22         6.58             41.05        4.30        2.2   \n",
       "2          309     9.49         7.42             48.39        4.41        2.6   \n",
       "5          290    11.50         2.17             44.54       22.02        1.6   \n",
       "9          238     2.15        10.66             43.91        5.00        9.2   \n",
       "10         250     1.53        10.20             48.89        6.12        4.1   \n",
       "...        ...      ...          ...               ...         ...        ...   \n",
       "1218       241    10.16         1.66             40.60       19.10        2.9   \n",
       "1219        19     0.27         1.39              3.78        0.71        3.1   \n",
       "1220        96     6.84         5.23              4.32        1.89        1.2   \n",
       "1221       208    15.70         1.22             17.17       11.23        2.3   \n",
       "1222        67     1.22         2.28             14.30        4.43        2.0   \n",
       "\n",
       "      Saturated Fats (g)  Calcium (mg)  Iron, Fe (mg)  Potassium, K (mg)  ...  \\\n",
       "1                  1.898           279           6.04              126.0  ...   \n",
       "2                  2.275           299           6.59              138.0  ...   \n",
       "5                  2.313            14           0.91               76.0  ...   \n",
       "9                  0.630           684           4.89              127.0  ...   \n",
       "10                 0.000            20           2.76              165.0  ...   \n",
       "...                  ...           ...            ...                ...  ...   \n",
       "1218               1.507             4           0.78              572.0  ...   \n",
       "1219               0.053            62           0.90              327.0  ...   \n",
       "1220               1.486            70           1.15              269.0  ...   \n",
       "1221               2.452            30           0.49              309.0  ...   \n",
       "1222               0.244             3           0.27              132.0  ...   \n",
       "\n",
       "      Carotene, alpha (mcg)  Lycopene (mcg)  Lutein + Zeaxanthin (mcg)  \\\n",
       "1                       0.0             0.0                       63.0   \n",
       "2                       0.0             0.0                       66.0   \n",
       "5                       0.0             1.0                       42.0   \n",
       "9                       0.0             0.0                       25.0   \n",
       "10                      0.0             0.0                       88.0   \n",
       "...                     ...             ...                        ...   \n",
       "1218                  418.0             0.0                       29.0   \n",
       "1219                    0.0             0.0                     4204.0   \n",
       "1220                   12.0           313.0                     4097.0   \n",
       "1221                 2157.0             1.0                      162.0   \n",
       "1222                    6.0             0.0                      693.0   \n",
       "\n",
       "      Fatty acids, total monounsaturated (mg)  \\\n",
       "1                                      4530.0   \n",
       "2                                      5292.0   \n",
       "5                                      5797.0   \n",
       "9                                       393.0   \n",
       "10                                      290.0   \n",
       "...                                       ...   \n",
       "1218                                   4099.0   \n",
       "1219                                      7.0   \n",
       "1220                                   2402.0   \n",
       "1221                                   3498.0   \n",
       "1222                                    373.0   \n",
       "\n",
       "      Fatty acids, total polyunsaturated (mg)  20:5 n-3 (EPA) (mg)  \\\n",
       "1                                      1445.0                 12.0   \n",
       "2                                      1502.0                 13.0   \n",
       "5                                      2117.0                  0.0   \n",
       "9                                       973.0                  3.0   \n",
       "10                                      936.0                  0.0   \n",
       "...                                       ...                  ...   \n",
       "1218                                   4079.0                  0.0   \n",
       "1219                                    126.0                  0.0   \n",
       "1220                                   2112.0                  0.0   \n",
       "1221                                   9319.0                  0.0   \n",
       "1222                                    518.0                  0.0   \n",
       "\n",
       "      22:5 n-3 (DPA) (mg)  22:6 n-3 (DHA) (mg)  Caffeine (mg)  \\\n",
       "1                     0.0                  7.0            0.0   \n",
       "2                     0.0                  8.0            0.0   \n",
       "5                     0.0                  0.0            0.0   \n",
       "9                     0.0                  0.0            0.0   \n",
       "10                    0.0                  0.0            0.0   \n",
       "...                   ...                  ...            ...   \n",
       "1218                  0.0                  0.0            0.0   \n",
       "1219                  0.0                  0.0            0.0   \n",
       "1220                  0.0                  0.0            0.0   \n",
       "1221                  0.0                  1.0            0.0   \n",
       "1222                  0.0                  0.0            0.0   \n",
       "\n",
       "      Theobromine (mg)  \n",
       "1                  0.0  \n",
       "2                  0.0  \n",
       "5                  0.0  \n",
       "9                  0.0  \n",
       "10                 0.0  \n",
       "...                ...  \n",
       "1218               0.0  \n",
       "1219               0.0  \n",
       "1220               0.0  \n",
       "1221               0.0  \n",
       "1222               0.0  \n",
       "\n",
       "[1042 rows x 40 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       0.264550\n",
       "2       0.264550\n",
       "5       0.560000\n",
       "9       0.118750\n",
       "10      0.355556\n",
       "          ...   \n",
       "1218    1.058824\n",
       "1219    0.250000\n",
       "1220    0.750000\n",
       "1221    0.041000\n",
       "1222    0.171429\n",
       "Name: Price per Weight (£/100Gram), Length: 1042, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# command/\n",
    "#only normalising X not y. Is this right?\n",
    "# norm_X = preprocessing.normalize(X, axis=0) #collum instead of row\n",
    "# norm_X = pd.DataFrame(norm_X, columns = X.columns)\n",
    "# X=norm_X\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.ravel(y)\n",
    "X = X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "84/84 [==============================] - 0s 707us/step - loss: 67.4959 - mse: 67.4959\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 0s 702us/step - loss: 10.0482 - mse: 10.0482\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 0s 628us/step - loss: 67.7567 - mse: 67.7567\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 0s 624us/step - loss: 6.4437 - mse: 6.4437\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 0s 596us/step - loss: 90.2687 - mse: 90.2687\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 0s 602us/step - loss: 12.5588 - mse: 12.5588\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 0s 581us/step - loss: 211.6659 - mse: 211.6659\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 0s 568us/step - loss: 191.1374 - mse: 191.1374\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 0s 593us/step - loss: 29.7739 - mse: 29.7739\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 0s 562us/step - loss: 51.8667 - mse: 51.8667\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 0s 559us/step - loss: 6.0873 - mse: 6.0873\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 0s 565us/step - loss: 74.5007 - mse: 74.5007\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 0s 590us/step - loss: 4.0051 - mse: 4.0051\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 0s 572us/step - loss: 119.9102 - mse: 119.9102\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 0s 562us/step - loss: 73.0288 - mse: 73.0288\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 0s 565us/step - loss: 208.6112 - mse: 208.6112\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 0s 569us/step - loss: 34.1367 - mse: 34.1367\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 0s 566us/step - loss: 67.2694 - mse: 67.2694\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 0s 568us/step - loss: 3.1306 - mse: 3.1306\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 0s 569us/step - loss: 143.3249 - mse: 143.3249\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 0s 562us/step - loss: 42.6386 - mse: 42.6386\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 0s 563us/step - loss: 109.0443 - mse: 109.0443\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 0s 563us/step - loss: 52.2826 - mse: 52.2826\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 0s 559us/step - loss: 52.3622 - mse: 52.3622\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 0s 567us/step - loss: 62.1903 - mse: 62.1903\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 0s 559us/step - loss: 93.8287 - mse: 93.8287\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 0s 557us/step - loss: 36.0981 - mse: 36.0981\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 0s 554us/step - loss: 30.6289 - mse: 30.6289\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 0s 563us/step - loss: 46.9531 - mse: 46.9531\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 0s 623us/step - loss: 14.8937 - mse: 14.8937\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 0s 566us/step - loss: 288.8706 - mse: 288.8706\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 0s 573us/step - loss: 17.1719 - mse: 17.1719\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 0s 566us/step - loss: 75.4463 - mse: 75.4463\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 0s 561us/step - loss: 6.4177 - mse: 6.4177\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 0s 622us/step - loss: 15.0337 - mse: 15.0337\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 0s 569us/step - loss: 125.6173 - mse: 125.6173\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 0s 569us/step - loss: 74.9676 - mse: 74.9676\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 0s 566us/step - loss: 56.6120 - mse: 56.6120\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 0s 559us/step - loss: 134.3070 - mse: 134.3070\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 0s 563us/step - loss: 85.9992 - mse: 85.9992\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 0s 559us/step - loss: 30.3878 - mse: 30.3878\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 0s 596us/step - loss: 40.7038 - mse: 40.7038\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 0s 561us/step - loss: 182.0365 - mse: 182.0365\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 0s 578us/step - loss: 25.3544 - mse: 25.3544\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 0s 565us/step - loss: 9.3345 - mse: 9.3345\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 0s 579us/step - loss: 13.7972 - mse: 13.7972\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 0s 572us/step - loss: 44.5181 - mse: 44.5181\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 0s 574us/step - loss: 141.7127 - mse: 141.7127\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 0s 576us/step - loss: 88.3459 - mse: 88.3459\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 0s 576us/step - loss: 57.5879 - mse: 57.5879\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 0s 576us/step - loss: 7.5333 - mse: 7.5333\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 0s 571us/step - loss: 92.7528 - mse: 92.7528\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 0s 630us/step - loss: 54.2820 - mse: 54.2820\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 0s 552us/step - loss: 113.1852 - mse: 113.1852\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 0s 534us/step - loss: 120.5279 - mse: 120.5279\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 0s 552us/step - loss: 72.6728 - mse: 72.6728\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 0s 543us/step - loss: 28.4320 - mse: 28.4320\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 0s 557us/step - loss: 79.1826 - mse: 79.1826\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 0s 555us/step - loss: 23.4028 - mse: 23.4028\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 0s 575us/step - loss: 59.8466 - mse: 59.8466\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 0s 553us/step - loss: 13.1525 - mse: 13.1525\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 0s 556us/step - loss: 19.7320 - mse: 19.7320\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 0s 558us/step - loss: 17.1492 - mse: 17.1492\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 0s 558us/step - loss: 44.0399 - mse: 44.0399\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 0s 542us/step - loss: 46.7751 - mse: 46.7751\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 0s 547us/step - loss: 156.5380 - mse: 156.5380\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 0s 546us/step - loss: 46.9024 - mse: 46.9024\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 0s 565us/step - loss: 77.4434 - mse: 77.4434\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 0s 564us/step - loss: 100.3276 - mse: 100.3276\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 0s 558us/step - loss: 16.5004 - mse: 16.5004\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 0s 569us/step - loss: 16.7341 - mse: 16.7341\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 0s 569us/step - loss: 12.8356 - mse: 12.8356\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 0s 567us/step - loss: 26.6638 - mse: 26.6638\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 0s 541us/step - loss: 44.3745 - mse: 44.3745\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 0s 540us/step - loss: 18.5798 - mse: 18.5798\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 0s 546us/step - loss: 144.5439 - mse: 144.5439\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 0s 543us/step - loss: 23.5908 - mse: 23.5908\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 0s 569us/step - loss: 50.2982 - mse: 50.2982\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 0s 570us/step - loss: 37.8668 - mse: 37.8668\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 0s 568us/step - loss: 33.1335 - mse: 33.1335\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 0s 565us/step - loss: 25.6430 - mse: 25.6430\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 0s 573us/step - loss: 56.8574 - mse: 56.8574\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 0s 569us/step - loss: 112.4880 - mse: 112.4880\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 0s 562us/step - loss: 84.4902 - mse: 84.4902\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 0s 542us/step - loss: 85.5549 - mse: 85.5549\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 0s 528us/step - loss: 15.6177 - mse: 15.6177\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 0s 530us/step - loss: 31.0911 - mse: 31.0911\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 0s 527us/step - loss: 84.0961 - mse: 84.0961\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 0s 544us/step - loss: 15.6672 - mse: 15.6672\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 0s 541us/step - loss: 83.2437 - mse: 83.2437\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 0s 541us/step - loss: 22.1912 - mse: 22.1912\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 0s 531us/step - loss: 40.9786 - mse: 40.9786\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 0s 541us/step - loss: 10.9728 - mse: 10.9728\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 0s 540us/step - loss: 40.3593 - mse: 40.3593\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 0s 548us/step - loss: 197.9642 - mse: 197.9642\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 0s 532us/step - loss: 115.9971 - mse: 115.9971\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 0s 529us/step - loss: 9.0327 - mse: 9.0327\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 0s 550us/step - loss: 144.6254 - mse: 144.6254\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 0s 538us/step - loss: 19.2132 - mse: 19.2132\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 0s 542us/step - loss: 4.8822 - mse: 4.8822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff743ebaad0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Feature Scaling/preprocessing - normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Building model\n",
    "model = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "model.add(Dense(8, activation = 'relu', input_dim = 40))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "model.add(Dense(units = 16, activation = 'relu'))\n",
    "\n",
    "#avoids overfitting\n",
    "#https://keras.io/api/layers/regularization_layers/dropout/\n",
    "model.add(keras.layers.Dropout(0.1))\n",
    "\n",
    "\n",
    "# Adding the output layer\n",
    "\n",
    "model.add(Dense(units = 1))\n",
    "\n",
    "\n",
    "# Compiling the ANN\n",
    "model.compile(optimizer = 'adam', loss = 'mse', metrics=['mse'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "history=model.fit(X_train, y_train, batch_size = 10, epochs = 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters involved:\n",
    "    https://keras.io/api/models/model_training_apis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 740us/step - loss: 10.6173 - mse: 10.6173\n"
     ]
    }
   ],
   "source": [
    "#model evaluation\n",
    "mse = model.evaluate(X_test, y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE without tuning: 10.617317199707031\n"
     ]
    }
   ],
   "source": [
    "print('MSE without tuning: {}'.format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 8)                 328       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 489\n",
      "Trainable params: 489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/api/models/sequential/\n",
    "The Sequential Class, provides a training and inference features on this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "plt.plot(y_test, color = 'red', label = 'Real data')\n",
    "plt.plot(y_pred, color = 'blue', label = 'Predicted data')\n",
    "plt.title('Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cite: https://stackoverflow.com/questions/49008074/how-to-create-a-neural-network-for-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.9038135686309418\n",
      "Mean Squared Error: 10.617318365125161\n",
      "Root Mean Squared Error: 3.2584226805503858\n"
     ]
    }
   ],
   "source": [
    "#Why are my errors all of a sudden so because of overfitting?\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kerastuner\n",
    "from kerastuner import HyperModel\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('units', min_value=8, max_value=32, step=4), \n",
    "        activation = hp.Choice('dense_activation', \n",
    "                values=['relu', 'tanh','sigmoid']),#,'softmax']),\n",
    "        input_dim = 40\n",
    "        )\n",
    "             \n",
    "    )\n",
    "\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('units', min_value=8, max_value=32, step=4), \n",
    "        activation = hp.Choice('dense_activation', \n",
    "                values=['relu', 'tanh','sigmoid'])#,'softmax'])\n",
    "        )\n",
    "             \n",
    "    )\n",
    "    \n",
    "    model.add(\n",
    "        keras.layers.Dropout(\n",
    "            hp.Float(\n",
    "                    'dropout',\n",
    "                    min_value=0.0,\n",
    "                    max_value=0.1,\n",
    "                    step=0.01)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Adding the output layer\n",
    "    model.add(Dense(units = 1))\n",
    "\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        #optimizer = hp.Choice('dense_optimizer',\n",
    "                #values=['adam','SGD','rmsprop','adadelta'] ),\n",
    "        loss = 'mse',\n",
    "        metrics = ['mse']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_rs = kerastuner.tuners.RandomSearch(\n",
    "            build_model,\n",
    "            objective='mse',\n",
    "            max_trials=100,\n",
    "            executions_per_trial=2, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 100 Complete [00h 00m 01s]\n",
      "mse: 63.06694984436035\n",
      "\n",
      "Best mse So Far: 61.77278137207031\n",
      "Total elapsed time: 00h 02m 26s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner_rs.search(X_train, y_train, epochs=10) #, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 713us/step - loss: 0.8163 - mse: 0.8163\n"
     ]
    }
   ],
   "source": [
    "#best_model = tuner_rs.get_best_models(num_models=1)\n",
    "best_model = tuner_rs.get_best_models(num_models=1)[0]\n",
    "mse_rs = best_model.evaluate(X_test, y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 0.9064 - mse: 0.9064\n"
     ]
    }
   ],
   "source": [
    "loss, mse = best_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in ./untitled_project\n",
      "Showing 10 best trials\n",
      "Objective(name='mse', direction='min')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 32\n",
      "dense_activation: relu\n",
      "dropout: 0.03\n",
      "Score: 61.77278137207031\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 32\n",
      "dense_activation: relu\n",
      "dropout: 0.01\n",
      "Score: 61.845191955566406\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 32\n",
      "dense_activation: relu\n",
      "dropout: 0.0\n",
      "Score: 61.97382354736328\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 32\n",
      "dense_activation: relu\n",
      "dropout: 0.07\n",
      "Score: 61.9947509765625\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 28\n",
      "dense_activation: relu\n",
      "dropout: 0.07\n",
      "Score: 62.03857612609863\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 24\n",
      "dense_activation: relu\n",
      "dropout: 0.01\n",
      "Score: 62.04085922241211\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 28\n",
      "dense_activation: relu\n",
      "dropout: 0.06\n",
      "Score: 62.05224418640137\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 24\n",
      "dense_activation: relu\n",
      "dropout: 0.03\n",
      "Score: 62.06519889831543\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 28\n",
      "dense_activation: relu\n",
      "dropout: 0.1\n",
      "Score: 62.10881233215332\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 20\n",
      "dense_activation: relu\n",
      "dropout: 0.02\n",
      "Score: 62.12076187133789\n"
     ]
    }
   ],
   "source": [
    "tuner_rs.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
