{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import tensorflow\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>Food Group</th>\n",
       "      <th>Price (£)</th>\n",
       "      <th>Weight (GRAMS)</th>\n",
       "      <th>Price per Weight (£/100Gram)</th>\n",
       "      <th>Carbon Group</th>\n",
       "      <th>Land use (m2/100g)</th>\n",
       "      <th>GHG(kgco2eq/100g)</th>\n",
       "      <th>Water use (L/100g)</th>\n",
       "      <th>Acidifying emissions(kgSO2eq per 100g)</th>\n",
       "      <th>...</th>\n",
       "      <th>Carotene, alpha (mcg)</th>\n",
       "      <th>Lycopene (mcg)</th>\n",
       "      <th>Lutein + Zeaxanthin (mcg)</th>\n",
       "      <th>Fatty acids, total monounsaturated (mg)</th>\n",
       "      <th>Fatty acids, total polyunsaturated (mg)</th>\n",
       "      <th>20:5 n-3 (EPA) (mg)</th>\n",
       "      <th>22:5 n-3 (DPA) (mg)</th>\n",
       "      <th>22:6 n-3 (DHA) (mg)</th>\n",
       "      <th>Caffeine (mg)</th>\n",
       "      <th>Theobromine (mg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Waffles Buttermilk Frozen Ready-To-Heat</td>\n",
       "      <td>Baked Foods</td>\n",
       "      <td>1.50</td>\n",
       "      <td>567.0</td>\n",
       "      <td>0.264550</td>\n",
       "      <td>Bread products</td>\n",
       "      <td>0.3482</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>4530.0</td>\n",
       "      <td>1445.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Waffle Buttermilk Frozen Ready-To-Heat Toasted</td>\n",
       "      <td>Baked Foods</td>\n",
       "      <td>1.50</td>\n",
       "      <td>567.0</td>\n",
       "      <td>0.264550</td>\n",
       "      <td>Bread products</td>\n",
       "      <td>0.3482</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>5292.0</td>\n",
       "      <td>1502.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Dutch Apple Pie</td>\n",
       "      <td>Baked Foods</td>\n",
       "      <td>2.80</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>Bread products</td>\n",
       "      <td>0.3482</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>5797.0</td>\n",
       "      <td>2117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Bread White Wheat</td>\n",
       "      <td>Baked Foods</td>\n",
       "      <td>0.95</td>\n",
       "      <td>800.0</td>\n",
       "      <td>0.118750</td>\n",
       "      <td>Bread products</td>\n",
       "      <td>0.3482</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>393.0</td>\n",
       "      <td>973.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Bagels Wheat</td>\n",
       "      <td>Baked Foods</td>\n",
       "      <td>1.60</td>\n",
       "      <td>450.0</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>Bread products</td>\n",
       "      <td>0.3482</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1218</td>\n",
       "      <td>Plantain Fried</td>\n",
       "      <td>Vegetables</td>\n",
       "      <td>0.90</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1.058824</td>\n",
       "      <td>Other vegetables</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>...</td>\n",
       "      <td>418.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4099.0</td>\n",
       "      <td>4079.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1219</td>\n",
       "      <td>Romaine Lettuce Raw</td>\n",
       "      <td>Vegetables</td>\n",
       "      <td>1.00</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>Other vegetables</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4204.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>Palak Paneer</td>\n",
       "      <td>Vegetables</td>\n",
       "      <td>3.75</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>Cheese</td>\n",
       "      <td>8.0642</td>\n",
       "      <td>2.1240</td>\n",
       "      <td>473.5</td>\n",
       "      <td>0.014894</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>4097.0</td>\n",
       "      <td>2402.0</td>\n",
       "      <td>2112.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1221</td>\n",
       "      <td>Carrots Raw Salad</td>\n",
       "      <td>Vegetables</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.041000</td>\n",
       "      <td>Other vegetables</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>...</td>\n",
       "      <td>2157.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>3498.0</td>\n",
       "      <td>9319.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1222</td>\n",
       "      <td>Corn on the cob</td>\n",
       "      <td>Vegetables</td>\n",
       "      <td>1.50</td>\n",
       "      <td>875.0</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>Other vegetables</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>693.0</td>\n",
       "      <td>373.0</td>\n",
       "      <td>518.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1042 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name   Food Group  Price (£)  \\\n",
       "1            Waffles Buttermilk Frozen Ready-To-Heat  Baked Foods       1.50   \n",
       "2     Waffle Buttermilk Frozen Ready-To-Heat Toasted  Baked Foods       1.50   \n",
       "5                                    Dutch Apple Pie  Baked Foods       2.80   \n",
       "9                                  Bread White Wheat  Baked Foods       0.95   \n",
       "10                                      Bagels Wheat  Baked Foods       1.60   \n",
       "...                                              ...          ...        ...   \n",
       "1218                                 Plantain Fried    Vegetables       0.90   \n",
       "1219                             Romaine Lettuce Raw   Vegetables       1.00   \n",
       "1220                                    Palak Paneer   Vegetables       3.75   \n",
       "1221                               Carrots Raw Salad   Vegetables       0.41   \n",
       "1222                                Corn on the cob    Vegetables       1.50   \n",
       "\n",
       "      Weight (GRAMS)  Price per Weight (£/100Gram)       Carbon Group  \\\n",
       "1              567.0                      0.264550    Bread products    \n",
       "2              567.0                      0.264550    Bread products    \n",
       "5              500.0                      0.560000    Bread products    \n",
       "9              800.0                      0.118750    Bread products    \n",
       "10             450.0                      0.355556    Bread products    \n",
       "...              ...                           ...                ...   \n",
       "1218            85.0                      1.058824  Other vegetables    \n",
       "1219           400.0                      0.250000  Other vegetables    \n",
       "1220           500.0                      0.750000            Cheese    \n",
       "1221          1000.0                      0.041000  Other vegetables    \n",
       "1222           875.0                      0.171429  Other vegetables    \n",
       "\n",
       "      Land use (m2/100g)  GHG(kgco2eq/100g)  Water use (L/100g)  \\\n",
       "1                 0.3482             0.1441                56.7   \n",
       "2                 0.3482             0.1441                56.7   \n",
       "5                 0.3482             0.1441                56.7   \n",
       "9                 0.3482             0.1441                56.7   \n",
       "10                0.3482             0.1441                56.7   \n",
       "...                  ...                ...                 ...   \n",
       "1218              0.0310             0.0455                 8.3   \n",
       "1219              0.0310             0.0455                 8.3   \n",
       "1220              8.0642             2.1240               473.5   \n",
       "1221              0.0310             0.0455                 8.3   \n",
       "1222              0.0310             0.0455                 8.3   \n",
       "\n",
       "      Acidifying emissions(kgSO2eq per 100g)  ...  Carotene, alpha (mcg)  \\\n",
       "1                                   0.001209  ...                    0.0   \n",
       "2                                   0.001209  ...                    0.0   \n",
       "5                                   0.001209  ...                    0.0   \n",
       "9                                   0.001209  ...                    0.0   \n",
       "10                                  0.001209  ...                    0.0   \n",
       "...                                      ...  ...                    ...   \n",
       "1218                                0.000531  ...                  418.0   \n",
       "1219                                0.000531  ...                    0.0   \n",
       "1220                                0.014894  ...                   12.0   \n",
       "1221                                0.000531  ...                 2157.0   \n",
       "1222                                0.000531  ...                    6.0   \n",
       "\n",
       "      Lycopene (mcg)  Lutein + Zeaxanthin (mcg)  \\\n",
       "1                0.0                       63.0   \n",
       "2                0.0                       66.0   \n",
       "5                1.0                       42.0   \n",
       "9                0.0                       25.0   \n",
       "10               0.0                       88.0   \n",
       "...              ...                        ...   \n",
       "1218             0.0                       29.0   \n",
       "1219             0.0                     4204.0   \n",
       "1220           313.0                     4097.0   \n",
       "1221             1.0                      162.0   \n",
       "1222             0.0                      693.0   \n",
       "\n",
       "      Fatty acids, total monounsaturated (mg)  \\\n",
       "1                                      4530.0   \n",
       "2                                      5292.0   \n",
       "5                                      5797.0   \n",
       "9                                       393.0   \n",
       "10                                      290.0   \n",
       "...                                       ...   \n",
       "1218                                   4099.0   \n",
       "1219                                      7.0   \n",
       "1220                                   2402.0   \n",
       "1221                                   3498.0   \n",
       "1222                                    373.0   \n",
       "\n",
       "      Fatty acids, total polyunsaturated (mg)  20:5 n-3 (EPA) (mg)  \\\n",
       "1                                      1445.0                 12.0   \n",
       "2                                      1502.0                 13.0   \n",
       "5                                      2117.0                  0.0   \n",
       "9                                       973.0                  3.0   \n",
       "10                                      936.0                  0.0   \n",
       "...                                       ...                  ...   \n",
       "1218                                   4079.0                  0.0   \n",
       "1219                                    126.0                  0.0   \n",
       "1220                                   2112.0                  0.0   \n",
       "1221                                   9319.0                  0.0   \n",
       "1222                                    518.0                  0.0   \n",
       "\n",
       "      22:5 n-3 (DPA) (mg)  22:6 n-3 (DHA) (mg)  Caffeine (mg)  \\\n",
       "1                     0.0                  7.0            0.0   \n",
       "2                     0.0                  8.0            0.0   \n",
       "5                     0.0                  0.0            0.0   \n",
       "9                     0.0                  0.0            0.0   \n",
       "10                    0.0                  0.0            0.0   \n",
       "...                   ...                  ...            ...   \n",
       "1218                  0.0                  0.0            0.0   \n",
       "1219                  0.0                  0.0            0.0   \n",
       "1220                  0.0                  0.0            0.0   \n",
       "1221                  0.0                  1.0            0.0   \n",
       "1222                  0.0                  0.0            0.0   \n",
       "\n",
       "      Theobromine (mg)  \n",
       "1                  0.0  \n",
       "2                  0.0  \n",
       "5                  0.0  \n",
       "9                  0.0  \n",
       "10                 0.0  \n",
       "...                ...  \n",
       "1218               0.0  \n",
       "1219               0.0  \n",
       "1220               0.0  \n",
       "1221               0.0  \n",
       "1222               0.0  \n",
       "\n",
       "[1042 rows x 51 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nutrition = pd.read_pickle(\"./Nutrition_Full_Features.pkl\")\n",
    "nutrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data for features\n",
    "X = nutrition.iloc[:, 6:]\n",
    "y = nutrition.iloc[:, 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Land use (m2/100g)</th>\n",
       "      <th>GHG(kgco2eq/100g)</th>\n",
       "      <th>Water use (L/100g)</th>\n",
       "      <th>Acidifying emissions(kgSO2eq per 100g)</th>\n",
       "      <th>Eutr emissions  (kg PO43-eq per 100g)</th>\n",
       "      <th>Calories</th>\n",
       "      <th>Fat (g)</th>\n",
       "      <th>Protein (g)</th>\n",
       "      <th>Carbohydrate (g)</th>\n",
       "      <th>Sugars (g)</th>\n",
       "      <th>...</th>\n",
       "      <th>Carotene, alpha (mcg)</th>\n",
       "      <th>Lycopene (mcg)</th>\n",
       "      <th>Lutein + Zeaxanthin (mcg)</th>\n",
       "      <th>Fatty acids, total monounsaturated (mg)</th>\n",
       "      <th>Fatty acids, total polyunsaturated (mg)</th>\n",
       "      <th>20:5 n-3 (EPA) (mg)</th>\n",
       "      <th>22:5 n-3 (DPA) (mg)</th>\n",
       "      <th>22:6 n-3 (DHA) (mg)</th>\n",
       "      <th>Caffeine (mg)</th>\n",
       "      <th>Theobromine (mg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.3482</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>273</td>\n",
       "      <td>9.22</td>\n",
       "      <td>6.58</td>\n",
       "      <td>41.05</td>\n",
       "      <td>4.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>4530.0</td>\n",
       "      <td>1445.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.3482</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>309</td>\n",
       "      <td>9.49</td>\n",
       "      <td>7.42</td>\n",
       "      <td>48.39</td>\n",
       "      <td>4.41</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>5292.0</td>\n",
       "      <td>1502.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.3482</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>290</td>\n",
       "      <td>11.50</td>\n",
       "      <td>2.17</td>\n",
       "      <td>44.54</td>\n",
       "      <td>22.02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>5797.0</td>\n",
       "      <td>2117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.3482</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>238</td>\n",
       "      <td>2.15</td>\n",
       "      <td>10.66</td>\n",
       "      <td>43.91</td>\n",
       "      <td>5.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>393.0</td>\n",
       "      <td>973.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.3482</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>56.7</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>250</td>\n",
       "      <td>1.53</td>\n",
       "      <td>10.20</td>\n",
       "      <td>48.89</td>\n",
       "      <td>6.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1218</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>241</td>\n",
       "      <td>10.16</td>\n",
       "      <td>1.66</td>\n",
       "      <td>40.60</td>\n",
       "      <td>19.10</td>\n",
       "      <td>...</td>\n",
       "      <td>418.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4099.0</td>\n",
       "      <td>4079.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1219</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>19</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.39</td>\n",
       "      <td>3.78</td>\n",
       "      <td>0.71</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4204.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>8.0642</td>\n",
       "      <td>2.1240</td>\n",
       "      <td>473.5</td>\n",
       "      <td>0.014894</td>\n",
       "      <td>0.008875</td>\n",
       "      <td>96</td>\n",
       "      <td>6.84</td>\n",
       "      <td>5.23</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.89</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>4097.0</td>\n",
       "      <td>2402.0</td>\n",
       "      <td>2112.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1221</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>208</td>\n",
       "      <td>15.70</td>\n",
       "      <td>1.22</td>\n",
       "      <td>17.17</td>\n",
       "      <td>11.23</td>\n",
       "      <td>...</td>\n",
       "      <td>2157.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>3498.0</td>\n",
       "      <td>9319.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1222</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>67</td>\n",
       "      <td>1.22</td>\n",
       "      <td>2.28</td>\n",
       "      <td>14.30</td>\n",
       "      <td>4.43</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>693.0</td>\n",
       "      <td>373.0</td>\n",
       "      <td>518.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1042 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Land use (m2/100g)  GHG(kgco2eq/100g)  Water use (L/100g)  \\\n",
       "1                 0.3482             0.1441                56.7   \n",
       "2                 0.3482             0.1441                56.7   \n",
       "5                 0.3482             0.1441                56.7   \n",
       "9                 0.3482             0.1441                56.7   \n",
       "10                0.3482             0.1441                56.7   \n",
       "...                  ...                ...                 ...   \n",
       "1218              0.0310             0.0455                 8.3   \n",
       "1219              0.0310             0.0455                 8.3   \n",
       "1220              8.0642             2.1240               473.5   \n",
       "1221              0.0310             0.0455                 8.3   \n",
       "1222              0.0310             0.0455                 8.3   \n",
       "\n",
       "      Acidifying emissions(kgSO2eq per 100g)  \\\n",
       "1                                   0.001209   \n",
       "2                                   0.001209   \n",
       "5                                   0.001209   \n",
       "9                                   0.001209   \n",
       "10                                  0.001209   \n",
       "...                                      ...   \n",
       "1218                                0.000531   \n",
       "1219                                0.000531   \n",
       "1220                                0.014894   \n",
       "1221                                0.000531   \n",
       "1222                                0.000531   \n",
       "\n",
       "      Eutr emissions  (kg PO43-eq per 100g)  Calories  Fat (g)  Protein (g)  \\\n",
       "1                                  0.000706       273     9.22         6.58   \n",
       "2                                  0.000706       309     9.49         7.42   \n",
       "5                                  0.000706       290    11.50         2.17   \n",
       "9                                  0.000706       238     2.15        10.66   \n",
       "10                                 0.000706       250     1.53        10.20   \n",
       "...                                     ...       ...      ...          ...   \n",
       "1218                               0.000186       241    10.16         1.66   \n",
       "1219                               0.000186        19     0.27         1.39   \n",
       "1220                               0.008875        96     6.84         5.23   \n",
       "1221                               0.000186       208    15.70         1.22   \n",
       "1222                               0.000186        67     1.22         2.28   \n",
       "\n",
       "      Carbohydrate (g)  Sugars (g)  ...  Carotene, alpha (mcg)  \\\n",
       "1                41.05        4.30  ...                    0.0   \n",
       "2                48.39        4.41  ...                    0.0   \n",
       "5                44.54       22.02  ...                    0.0   \n",
       "9                43.91        5.00  ...                    0.0   \n",
       "10               48.89        6.12  ...                    0.0   \n",
       "...                ...         ...  ...                    ...   \n",
       "1218             40.60       19.10  ...                  418.0   \n",
       "1219              3.78        0.71  ...                    0.0   \n",
       "1220              4.32        1.89  ...                   12.0   \n",
       "1221             17.17       11.23  ...                 2157.0   \n",
       "1222             14.30        4.43  ...                    6.0   \n",
       "\n",
       "      Lycopene (mcg)  Lutein + Zeaxanthin (mcg)  \\\n",
       "1                0.0                       63.0   \n",
       "2                0.0                       66.0   \n",
       "5                1.0                       42.0   \n",
       "9                0.0                       25.0   \n",
       "10               0.0                       88.0   \n",
       "...              ...                        ...   \n",
       "1218             0.0                       29.0   \n",
       "1219             0.0                     4204.0   \n",
       "1220           313.0                     4097.0   \n",
       "1221             1.0                      162.0   \n",
       "1222             0.0                      693.0   \n",
       "\n",
       "      Fatty acids, total monounsaturated (mg)  \\\n",
       "1                                      4530.0   \n",
       "2                                      5292.0   \n",
       "5                                      5797.0   \n",
       "9                                       393.0   \n",
       "10                                      290.0   \n",
       "...                                       ...   \n",
       "1218                                   4099.0   \n",
       "1219                                      7.0   \n",
       "1220                                   2402.0   \n",
       "1221                                   3498.0   \n",
       "1222                                    373.0   \n",
       "\n",
       "      Fatty acids, total polyunsaturated (mg)  20:5 n-3 (EPA) (mg)  \\\n",
       "1                                      1445.0                 12.0   \n",
       "2                                      1502.0                 13.0   \n",
       "5                                      2117.0                  0.0   \n",
       "9                                       973.0                  3.0   \n",
       "10                                      936.0                  0.0   \n",
       "...                                       ...                  ...   \n",
       "1218                                   4079.0                  0.0   \n",
       "1219                                    126.0                  0.0   \n",
       "1220                                   2112.0                  0.0   \n",
       "1221                                   9319.0                  0.0   \n",
       "1222                                    518.0                  0.0   \n",
       "\n",
       "      22:5 n-3 (DPA) (mg)  22:6 n-3 (DHA) (mg)  Caffeine (mg)  \\\n",
       "1                     0.0                  7.0            0.0   \n",
       "2                     0.0                  8.0            0.0   \n",
       "5                     0.0                  0.0            0.0   \n",
       "9                     0.0                  0.0            0.0   \n",
       "10                    0.0                  0.0            0.0   \n",
       "...                   ...                  ...            ...   \n",
       "1218                  0.0                  0.0            0.0   \n",
       "1219                  0.0                  0.0            0.0   \n",
       "1220                  0.0                  0.0            0.0   \n",
       "1221                  0.0                  1.0            0.0   \n",
       "1222                  0.0                  0.0            0.0   \n",
       "\n",
       "      Theobromine (mg)  \n",
       "1                  0.0  \n",
       "2                  0.0  \n",
       "5                  0.0  \n",
       "9                  0.0  \n",
       "10                 0.0  \n",
       "...                ...  \n",
       "1218               0.0  \n",
       "1219               0.0  \n",
       "1220               0.0  \n",
       "1221               0.0  \n",
       "1222               0.0  \n",
       "\n",
       "[1042 rows x 45 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       0.264550\n",
       "2       0.264550\n",
       "5       0.560000\n",
       "9       0.118750\n",
       "10      0.355556\n",
       "          ...   \n",
       "1218    1.058824\n",
       "1219    0.250000\n",
       "1220    0.750000\n",
       "1221    0.041000\n",
       "1222    0.171429\n",
       "Name: Price per Weight (£/100Gram), Length: 1042, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# command/\n",
    "#only normalising X not y. Is this right?\n",
    "# norm_X = preprocessing.normalize(X, axis=0) #collum instead of row\n",
    "# norm_X = pd.DataFrame(norm_X, columns = X.columns)\n",
    "# X=norm_X\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.ravel(y)\n",
    "X = X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 765us/step - loss: 143.0537 - mse: 143.0537\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 0s 632us/step - loss: 34.6006 - mse: 34.6006\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 0s 591us/step - loss: 84.9611 - mse: 84.9611\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 0s 588us/step - loss: 50.6914 - mse: 50.6914\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 0s 623us/step - loss: 93.6605 - mse: 93.6605\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 0s 798us/step - loss: 107.5461 - mse: 107.5461\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 0s 660us/step - loss: 36.6629 - mse: 36.6629\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 0s 619us/step - loss: 107.5857 - mse: 107.5857\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 0s 621us/step - loss: 12.7765 - mse: 12.7765\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 0s 553us/step - loss: 16.8525 - mse: 16.8525\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 0s 559us/step - loss: 5.4224 - mse: 5.4224\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 0s 561us/step - loss: 242.0308 - mse: 242.0308\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 0s 564us/step - loss: 131.7585 - mse: 131.7585\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 0s 566us/step - loss: 164.3174 - mse: 164.3174\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 0s 569us/step - loss: 9.6710 - mse: 9.6710\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 0s 564us/step - loss: 66.3231 - mse: 66.3231\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 0s 613us/step - loss: 23.7263 - mse: 23.7263\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 0s 592us/step - loss: 11.5600 - mse: 11.5600\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 0s 549us/step - loss: 47.2637 - mse: 47.2637\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 0s 556us/step - loss: 87.7880 - mse: 87.7880\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 0s 581us/step - loss: 296.9496 - mse: 296.9496\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 0s 590us/step - loss: 46.2843 - mse: 46.2843\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 0s 581us/step - loss: 16.1596 - mse: 16.1596\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 0s 584us/step - loss: 27.4480 - mse: 27.4480\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 0s 626us/step - loss: 42.2882 - mse: 42.2882\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 0s 562us/step - loss: 39.2799 - mse: 39.2799\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 0s 568us/step - loss: 236.7607 - mse: 236.7607\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 0s 588us/step - loss: 142.9108 - mse: 142.9108\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 0s 756us/step - loss: 8.8878 - mse: 8.8878\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 0s 612us/step - loss: 56.5199 - mse: 56.5199\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 0s 574us/step - loss: 42.3417 - mse: 42.3417\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 0s 565us/step - loss: 29.7718 - mse: 29.7718\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 0s 561us/step - loss: 41.7707 - mse: 41.7707\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 0s 565us/step - loss: 161.2539 - mse: 161.2539\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 0s 591us/step - loss: 67.3028 - mse: 67.3028\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 0s 639us/step - loss: 36.2161 - mse: 36.2161\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 0s 589us/step - loss: 73.1663 - mse: 73.1663\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 0s 638us/step - loss: 16.5729 - mse: 16.5729\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 0s 627us/step - loss: 32.8455 - mse: 32.8455\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 0s 621us/step - loss: 4.8617 - mse: 4.8617\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 0s 566us/step - loss: 17.1156 - mse: 17.1156\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 0s 580us/step - loss: 63.3063 - mse: 63.3063\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 0s 588us/step - loss: 34.6453 - mse: 34.6453\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 0s 591us/step - loss: 21.9315 - mse: 21.9315\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 0s 577us/step - loss: 49.8217 - mse: 49.8217\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 0s 699us/step - loss: 28.9448 - mse: 28.9448\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 0s 586us/step - loss: 30.2365 - mse: 30.2365\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 0s 592us/step - loss: 32.3188 - mse: 32.3188\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 0s 591us/step - loss: 52.8194 - mse: 52.8194\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 0s 603us/step - loss: 72.0027 - mse: 72.0027\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 0s 599us/step - loss: 67.0593 - mse: 67.0593\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 0s 578us/step - loss: 16.2194 - mse: 16.2194\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 0s 702us/step - loss: 12.0772 - mse: 12.0772\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 0s 778us/step - loss: 74.3035 - mse: 74.3035\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 0s 591us/step - loss: 43.1288 - mse: 43.1288\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 0s 572us/step - loss: 14.8492 - mse: 14.8492\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 0s 573us/step - loss: 164.3018 - mse: 164.3018\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 0s 777us/step - loss: 28.2286 - mse: 28.2286\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 0s 568us/step - loss: 27.0975 - mse: 27.0975\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 0s 617us/step - loss: 17.7337 - mse: 17.7337\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 0s 583us/step - loss: 31.2444 - mse: 31.2444\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 0s 544us/step - loss: 16.9706 - mse: 16.9706\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 0s 577us/step - loss: 67.2687 - mse: 67.2687\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 0s 562us/step - loss: 15.3302 - mse: 15.3302\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 0s 562us/step - loss: 11.0514 - mse: 11.0514\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 0s 567us/step - loss: 50.2432 - mse: 50.2432\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 0s 567us/step - loss: 84.8105 - mse: 84.8105\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 0s 561us/step - loss: 41.1238 - mse: 41.1238\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 0s 552us/step - loss: 47.2230 - mse: 47.2230\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 0s 622us/step - loss: 122.1119 - mse: 122.1119\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 0s 581us/step - loss: 20.6732 - mse: 20.6732\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 0s 566us/step - loss: 62.9909 - mse: 62.9909\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 0s 603us/step - loss: 4.3402 - mse: 4.3402\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 0s 595us/step - loss: 44.6008 - mse: 44.6008\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 0s 563us/step - loss: 34.3057 - mse: 34.3057\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 0s 565us/step - loss: 31.6614 - mse: 31.6614\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 0s 555us/step - loss: 88.2007 - mse: 88.2007\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 0s 587us/step - loss: 15.7804 - mse: 15.7804\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 0s 556us/step - loss: 43.6780 - mse: 43.6780\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 0s 562us/step - loss: 18.1181 - mse: 18.1181\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 0s 541us/step - loss: 11.0054 - mse: 11.0054\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 0s 549us/step - loss: 6.3091 - mse: 6.3091\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 0s 592us/step - loss: 26.0841 - mse: 26.0841\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 0s 552us/step - loss: 106.0332 - mse: 106.0332\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 0s 564us/step - loss: 78.4576 - mse: 78.4576\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 0s 589us/step - loss: 103.3132 - mse: 103.3132\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 0s 549us/step - loss: 53.0636 - mse: 53.0636\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 0s 526us/step - loss: 5.6179 - mse: 5.6179\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 0s 524us/step - loss: 52.7989 - mse: 52.7989\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 0s 538us/step - loss: 21.2255 - mse: 21.2255\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 0s 560us/step - loss: 94.3134 - mse: 94.3134\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 0s 543us/step - loss: 172.9777 - mse: 172.9777\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 0s 550us/step - loss: 55.2230 - mse: 55.2230\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 0s 715us/step - loss: 30.2573 - mse: 30.2573\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 0s 576us/step - loss: 58.8747 - mse: 58.8747\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 0s 574us/step - loss: 22.1484 - mse: 22.1484\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 0s 566us/step - loss: 87.4393 - mse: 87.4393\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 0s 546us/step - loss: 90.0452 - mse: 90.0452\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 0s 613us/step - loss: 84.4238 - mse: 84.4238\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 0s 581us/step - loss: 12.5544 - mse: 12.5544\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Feature Scaling/preprocessing - normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Building model\n",
    "model = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "model.add(Dense(8, activation = 'relu', input_dim = 45))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "model.add(Dense(units = 16, activation = 'relu'))\n",
    "\n",
    "#avoids overfitting\n",
    "#https://keras.io/api/layers/regularization_layers/dropout/\n",
    "model.add(keras.layers.Dropout(0.1))\n",
    "\n",
    "\n",
    "# Adding the output layer\n",
    "\n",
    "model.add(Dense(units = 1))\n",
    "\n",
    "\n",
    "# Compiling the ANN\n",
    "model.compile(optimizer = 'adam', loss = 'mse', metrics=['mse'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "history=model.fit(X_train, y_train, batch_size = 10, epochs = 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters involved:\n",
    "    https://keras.io/api/models/model_training_apis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 736us/step - loss: 5.0372 - mse: 5.0372\n"
     ]
    }
   ],
   "source": [
    "#model evaluation\n",
    "mse = model.evaluate(X_test, y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE without tuning: 5.037153244018555\n"
     ]
    }
   ],
   "source": [
    "print('MSE without tuning: {}'.format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 8)                 368       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 529\n",
      "Trainable params: 529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/api/models/sequential/\n",
    "The Sequential Class, provides a training and inference features on this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "plt.plot(y_test, color = 'red', label = 'Real data')\n",
    "plt.plot(y_pred, color = 'blue', label = 'Predicted data')\n",
    "plt.title('Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cite: https://stackoverflow.com/questions/49008074/how-to-create-a-neural-network-for-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.9652919475815259\n",
      "Mean Squared Error: 5.037153464082974\n",
      "Root Mean Squared Error: 2.244360368586777\n"
     ]
    }
   ],
   "source": [
    "#Why are my errors all of a sudden so because of overfitting?\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kerastuner\n",
    "from kerastuner import HyperModel\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('units', min_value=8, max_value=32, step=4), \n",
    "        activation = hp.Choice('dense_activation', \n",
    "                values=['relu', 'tanh','sigmoid']),#,'softmax']),\n",
    "        input_dim = 40\n",
    "        )\n",
    "             \n",
    "    )\n",
    "\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('units', min_value=8, max_value=32, step=4), \n",
    "        activation = hp.Choice('dense_activation', \n",
    "                values=['relu', 'tanh','sigmoid'])#,'softmax'])\n",
    "        )\n",
    "             \n",
    "    )\n",
    "    \n",
    "    model.add(\n",
    "        keras.layers.Dropout(\n",
    "            hp.Float(\n",
    "                    'dropout',\n",
    "                    min_value=0.0,\n",
    "                    max_value=0.1,\n",
    "                    step=0.01)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Adding the output layer\n",
    "    model.add(Dense(units = 1))\n",
    "\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        #optimizer = hp.Choice('dense_optimizer',\n",
    "                #values=['adam','SGD','rmsprop','adadelta'] ),\n",
    "        loss = 'mse',\n",
    "        metrics = ['mse']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_rs = kerastuner.tuners.RandomSearch(\n",
    "            build_model,\n",
    "            objective='mse',\n",
    "            max_trials=100,\n",
    "            executions_per_trial=2, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "units             |8                 |?                 \n",
      "dense_activation  |tanh              |?                 \n",
      "dropout           |0.02              |?                 \n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:754 train_step\n        y_pred = self(x, training=True)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py:259 assert_input_compatibility\n        ' but received input with shape ' + display_shape(x.shape))\n\n    ValueError: Input 0 of layer sequential is incompatible with the layer: expected axis -1 of input shape to have value 40 but received input with shape (None, 45)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f9e9c66ebd1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtuner_rs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#, verbose=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/kerastuner/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/kerastuner/engine/multi_execution_tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'callbacks'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_values\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'min'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/kerastuner/engine/tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[0;34m(self, trial, fit_args, fit_kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \"\"\"\n\u001b[1;32m    140\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:754 train_step\n        y_pred = self(x, training=True)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py:259 assert_input_compatibility\n        ' but received input with shape ' + display_shape(x.shape))\n\n    ValueError: Input 0 of layer sequential is incompatible with the layer: expected axis -1 of input shape to have value 40 but received input with shape (None, 45)\n"
     ]
    }
   ],
   "source": [
    "tuner_rs.search(X_train, y_train, epochs=10) #, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model = tuner_rs.get_best_models(num_models=1)\n",
    "best_model = tuner_rs.get_best_models(num_models=1)[0]\n",
    "mse_rs = best_model.evaluate(X_test, y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, mse = best_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_rs.results_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
